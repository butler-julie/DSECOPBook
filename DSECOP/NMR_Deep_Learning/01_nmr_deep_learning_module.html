

<!DOCTYPE html>


<html >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>NMR Spectrum Analysis Using Artificial Neural Networks &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'DSECOP/NMR_Deep_Learning/01_nmr_deep_learning_module';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="None"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Table of Contents
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Exploratory Data Analysis</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Exploratory_Data_Analysis/README.html">Exploratory Data Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Exploratory_Data_Analysis/01_dataset_cleaning.html">Exploratory Data Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Exploratory_Data_Analysis/02_preprocessing_techniques.html">Preprocessing Techniques for Machine Learning</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FDSECOP/NMR_Deep_Learning/01_nmr_deep_learning_module.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/DSECOP/NMR_Deep_Learning/01_nmr_deep_learning_module.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>NMR Spectrum Analysis Using Artificial Neural Networks</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">NMR Spectrum Analysis Using Artificial Neural Networks</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#authored-by-sebastian-w-atalla">Authored by Sebastian W. Atalla</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unc-chapel-hill-dept-of-physics-astronomy">UNC Chapel Hill | Dept. of Physics &amp; Astronomy</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#background"><strong>Background</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-applications">Deep Learning Applications</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disclaimer"><strong>Disclaimer</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparation">Preparation</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-estimation-of-fid-frequency-content-from-fft"><strong>Exercise 1</strong> Estimation of FID Frequency Content from FFT</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#signal-data">Signal Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-neural-network">The Neural Network</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1">Exercise 1</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#layers"><strong>Layers</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions"><strong>Activation Functions</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-function"><strong>Linear Function</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid-function"><strong>Sigmoid Function</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-function"><strong>Softmax Function</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#rectified-linear-unit-relu-function"><strong>Rectified Linear Unit (ReLU) Function</strong></a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization"><strong>Regularization</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#l1-and-l2-regularizers"><strong>L1 and L2 Regularizers</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout"><strong>Dropout</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#early-stopping"><strong>Early Stopping</strong></a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizer"><strong>Optimizer</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent"><strong>Stochastic Gradient Descent</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate"><strong>Learning Rate</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#momentum"><strong>Momentum</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ann-predictions">ANN Predictions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion">Discussion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-estimation-of-lorentzian-curve-fitting-parameters-from-fid-frequency-spectra"><strong>Exercise 2</strong> Estimation of Lorentzian Curve Fitting Parameters from FID Frequency Spectra</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#curve-fitting-of-nmr-data">Curve Fitting of NMR Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lorentzian-distribution"><strong>Lorentzian Distribution</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-distribution"><strong>Gaussian Distribution</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#voigt-profile"><strong>Voigt Profile</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lorentzian-fit">Lorentzian Fit</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">The Neural Network</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2">Exercise 2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">ANN Predictions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Discussion</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <p><a href="https://colab.research.google.com/github/GDS-Education-Community-of-Practice/DSECOP/blob/main/NMR_Deep_Learning/01_nmr_deep_learning_module.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<section class="tex2jax_ignore mathjax_ignore" id="nmr-spectrum-analysis-using-artificial-neural-networks">
<h1>NMR Spectrum Analysis Using Artificial Neural Networks<a class="headerlink" href="#nmr-spectrum-analysis-using-artificial-neural-networks" title="Permalink to this headline">#</a></h1>
<hr class="docutils" />
<section id="authored-by-sebastian-w-atalla">
<h2>Authored by Sebastian W. Atalla<a class="headerlink" href="#authored-by-sebastian-w-atalla" title="Permalink to this headline">#</a></h2>
<section id="unc-chapel-hill-dept-of-physics-astronomy">
<h3>UNC Chapel Hill | Dept. of Physics &amp; Astronomy<a class="headerlink" href="#unc-chapel-hill-dept-of-physics-astronomy" title="Permalink to this headline">#</a></h3>
<p><em>The content of this notebook is computationally expensive, and is best experienced with a GPU-accelerated runtime. Such a runtime is available via Google Colab. To open this notebook in a Colab environment, follow the badge above.</em></p>
</section>
</section>
<section id="background">
<h2><strong>Background</strong><a class="headerlink" href="#background" title="Permalink to this headline">#</a></h2>
<p>The Fourier transform is one of the most important concepts in signal processing. It translates an acquired signal between its time and frequency domain representations, allowing for the decomposition of a signal into its constituent frequencies, or construction of a signal from those frequencies.</p>
<p>The Fourier transform, <span class="math notranslate nohighlight">\(F(k)\)</span>, and its inverse transformation, <span class="math notranslate nohighlight">\(f(x)\)</span>, are generally given by</p>
<p>\begin{equation}
F(k) = \int_{-\infty}^{\infty} f(x) , e^{-2\pi i k x} , dx<br />
\qquad \qquad
f(x) = \int_{-\infty}^{\infty} F(k) , e^{2\pi i k x} , dk
\end{equation}</p>
<p>respectively. The Fourier transform as as defined above may be naively computed via the discrete Fourier transform (DFT) and its inverse transformation,</p>
<p>\begin{equation}
X_k = \sum_{n=0}^{N-1} x_n , e^{-i\frac{2\pi}{N}kn}
\qquad \qquad
x_n = \frac{1}{N} \sum_{k=0}^{N-1} X_k , e^{i\frac{2\pi}{N}kn}
\end{equation}</p>
<p>respectively; however, this is remarkably ineffificent compared to the modern fast Fourier transform (FFT) utilized in practically all computational domains.</p>
<p>The Fourier transform is used to great effect in nuclear magnetic resonance (NMR) spectroscopy, which exploits the fact that different atomic species resonate at different frequencies in a magnetic field as defined by their gyromagnetic ratio – a phenomenon known as spin precession. Faraday induction is used to detect the precession of the magnetization vector of the nuclear spins, which yields the magnetization amplitude as a function of time. The DFT decomposes this signal into discrete frequencies that may be interpreted in terms of chemical shift, which is a field-independent representation of a nucleus’s resonant frequency.</p>
<p>This same principle is utilized heavily in magnetic resonance imaging (MRI), in which images are reconstructed via a 2-dimensional FFT from data collected by the MRI machine in <em>k-space</em>, which represents spatial frequencies in the acquired image.</p>
</section>
<section id="deep-learning-applications">
<h2>Deep Learning Applications<a class="headerlink" href="#deep-learning-applications" title="Permalink to this headline">#</a></h2>
<p>Deep learning is a sub-discipline of machine learning that encompasses the concept of neural networks. Theoretically, neural networks scale better than traditional computational methods for large amounts of data. This is particularly important for signal processing, since some datasets (e.g., magnetic resonance imaging data) can be extremely large. Deep learning algorithms attempt to learn features about the data; however, in the context of traditional computation and the DFT, the features are intrinsic to the signal. Therein arises the point of this module – can a neural network estimate the frequency content of a signal?</p>
<section id="disclaimer">
<h3><strong>Disclaimer</strong><a class="headerlink" href="#disclaimer" title="Permalink to this headline">#</a></h3>
<p>The FFT algorithm is efficient and exact. Practically, the Fourier transform is a poor application of deep learning; however, the exactness of the computation (which provides a ground truth for the network’s predictions) and the litany of features that may be discerned from the data render it a good thought experiment for learning about neural net architecture and feature engineering.</p>
</section>
</section>
<section id="preparation">
<h2>Preparation<a class="headerlink" href="#preparation" title="Permalink to this headline">#</a></h2>
<p><strong>Execute these cells to initialize notebook</strong>. These cells import packages, define constants, and instantiate generators used in the following cells.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Computation</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">numpy.random</span> <span class="kn">import</span> <span class="n">default_rng</span>
<span class="kn">import</span> <span class="nn">scipy</span>
<span class="kn">import</span> <span class="nn">scipy.optimize</span>
<span class="kn">import</span> <span class="nn">scipy.signal</span>
<span class="kn">import</span> <span class="nn">scipy.integrate</span>

<span class="c1"># Visualization</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="kn">import</span> <span class="nn">matplotlib.patches</span> <span class="k">as</span> <span class="nn">mpatches</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sb</span>

<span class="c1">#  Machine learning</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow</span> <span class="kn">import</span> <span class="n">keras</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">line</span> <span class="mi">12</span>
<span class="g g-Whitespace">     </span><span class="mi">10</span> <span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>
<span class="g g-Whitespace">     </span><span class="mi">11</span> <span class="kn">import</span> <span class="nn">matplotlib.patches</span> <span class="k">as</span> <span class="nn">mpatches</span>
<span class="ne">---&gt; </span><span class="mi">12</span> <span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sb</span>
<span class="g g-Whitespace">     </span><span class="mi">14</span> <span class="c1">#  Machine learning</span>
<span class="g g-Whitespace">     </span><span class="mi">15</span> <span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;seaborn&#39;
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># If problems arise while editing NN parameters, run this cell to clear persistent graph data.</span>
<span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">backend</span><span class="o">.</span><span class="n">clear_session</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define a constant for pi since `np.pi` is clumsy, and it&#39;s preferable to not import it</span>
<span class="n">PI</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span>

<span class="c1"># Random generator</span>
<span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="exercise-1-estimation-of-fid-frequency-content-from-fft">
<h1><strong>Exercise 1</strong> Estimation of FID Frequency Content from FFT<a class="headerlink" href="#exercise-1-estimation-of-fid-frequency-content-from-fft" title="Permalink to this headline">#</a></h1>
<p>This exercise explores the fidelity of a neural network trained on an NMR signal and the frequency content from that signal’s FFT. The signals are simulated from the expression describing a free induction decay (FID), a waveform prevalent in the common single-pulse sequences used in NMR spectroscopy.</p>
<p>An FID may be described by</p>
<p>\begin{equation}
\text{FID} = \sin{(\omega_0 t)} , e^{-t/T_2^*}
\end{equation}</p>
<p>where <span class="math notranslate nohighlight">\(\omega_0\)</span> and <span class="math notranslate nohighlight">\(t\)</span> are of the usual formulation in signal processing, and <span class="math notranslate nohighlight">\(T_2^*\)</span> is the effective decay in transverse magnetization of the nucleus being measured.</p>
<p>The NMR spectrometer detects a signal as a series of voltages, which must be digitized by an analog-to-digital (ADC) converter, which binarizes the signal for use on a computer. Sampling by the ADC occurs in regular intervals, which yields the datapoints seen in the FID.</p>
<p>The sampling rate of the ADC must be fast enough that important information in the signal is not lost. The minimum sampling frequency, <span class="math notranslate nohighlight">\(f_{s_{min}}\)</span>, needed to render an accurate signal is</p>
<p>\begin{equation}
f_{s_{min}} \equiv f_{N} = 2f_{max}
\end{equation}</p>
<p>where <span class="math notranslate nohighlight">\(f_{N}\)</span> is the <em>Nyquist frequency</em> and <span class="math notranslate nohighlight">\(f_{max}\)</span> is the highest sampled frequency.</p>
<p>The Nyquist limit is problematic because NMR frequencies are on the order of megahertz. An ADC cannot digitize fast enough to capture this signal. The solution to this is <em>RF mixing</em>, in which two frequencies (typically the observed frequency and a reference frequency) are multiplied to yield sinusoids of sums and differences of the two frequencies,</p>
<p>\begin{equation}
A\cos{(\omega_0 t)} , \times , A\cos{(\omega_{rx} t)} =
\frac{1}{2}A \big[ \cos{ \big( (\omega_0 + \omega_{rx})t \big)}\big] + \cos{ \big( (\omega_0 - \omega_{rx})t \big)}\big]
\end{equation}</p>
<p>where <span class="math notranslate nohighlight">\(\omega_0\)</span> is the Larmor frequency, and <span class="math notranslate nohighlight">\(\omega_{rx}\)</span> is the reference frequency. After applying a low-pass filter, the signal of interest is now simply</p>
<p>\begin{equation}
S = \frac{1}{2}A\left[\cos{ \big( (\omega_0 - \omega_{rx})t \big)}\right] = \frac{1}{2}A\left[\cos{ (\omega_{\text{mix}})t)}\right]
\end{equation}</p>
<p>in which <span class="math notranslate nohighlight">\(\omega_{\text{mix}}\)</span> is a frequency that may now be sampled by the ADC.</p>
<section id="signal-data">
<h2>Signal Data<a class="headerlink" href="#signal-data" title="Permalink to this headline">#</a></h2>
<p>The FIDs will be simulated from an array of RF-mixed frequencies spanning 50 Hz to 1 kHz. A series of <span class="math notranslate nohighlight">\(T_2^*\)</span> relaxation times will also be used to emulate the spectra in various media (e.g. tissue, water, fat, etc.).</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># RF=mixed frequencies from 50 Hz to 1 kHz</span>
<span class="n">rf_mix_frequency</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">50</span><span class="p">,</span> <span class="mi">1000</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># T2 spin-spin relaxation times for proton in various media</span>
<span class="n">spin2_relaxation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="mi">25</span><span class="p">,</span> <span class="n">endpoint</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span> <span class="c1"># Seconds</span>

<span class="c1"># Nyquist limit for highest frequencu; satisfies all Nyquist limit reqs. for lower frequencies</span>
<span class="n">min_sampling_freq</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">rf_mix_frequency</span><span class="o">.</span><span class="n">max</span><span class="p">()</span> <span class="c1"># Hertz</span>

<span class="c1"># Sampling that satisfies the Nyquist limit for the highest RF-mixed frequency</span>
<span class="n">time</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">min_sampling_freq</span><span class="p">))</span> <span class="c1"># Seconds</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fid_sim</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">T2</span><span class="p">,</span> <span class="n">phase_shift</span> <span class="o">=</span> <span class="mi">0</span><span class="p">):</span>
   <span class="sd">&quot;&quot;&quot; Simulates a free induction decay (FID). &quot;&quot;&quot;</span>
   <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">PI</span> <span class="o">*</span> <span class="n">w0</span> <span class="o">*</span> <span class="n">t</span> <span class="o">+</span> <span class="n">phase_shift</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">t</span><span class="o">/</span><span class="n">T2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># At least one of the args needs to be Iterable to make a table; for simplicity, enforce it on all args</span>
<span class="k">def</span> <span class="nf">make_fid_table</span><span class="p">(</span><span class="n">frequency</span><span class="p">,</span> <span class="n">timepts</span><span class="p">,</span> <span class="n">relaxation</span><span class="p">):</span>
   <span class="sd">&quot;&quot;&quot; Constructs a table of simulated FIDs. &quot;&quot;&quot;</span>
   <span class="c1"># Use list for efficient append</span>
   <span class="n">fid_table</span> <span class="o">=</span> <span class="p">[]</span>

   <span class="c1"># `sz_dim` is the number of signals generated</span>
   <span class="n">sz_dim</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">frequency</span><span class="p">)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">relaxation</span><span class="p">)</span>

   <span class="c1"># Generate amplitude scaling and phase shifting factors to introduce more variance</span>
   <span class="n">amplitude_scale_table</span>   <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">sz_dim</span><span class="p">)</span>
   <span class="n">phase_shift_table</span>       <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="o">*</span><span class="n">PI</span><span class="p">,</span> <span class="n">sz_dim</span><span class="p">)</span>

   <span class="c1"># Iterate over each relaxation time for each frequency</span>
   <span class="k">for</span> <span class="n">w0</span> <span class="ow">in</span> <span class="n">frequency</span><span class="p">:</span>
      <span class="k">for</span> <span class="n">t2</span> <span class="ow">in</span> <span class="n">relaxation</span><span class="p">:</span>
         <span class="c1"># Randomly select an amplitude scaling factor and phase shift</span>
         <span class="n">amp</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">amplitude_scale_table</span><span class="p">)</span>
         <span class="n">phi</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">phase_shift_table</span><span class="p">)</span>

         <span class="c1"># Generate fid and append to the FID table</span>
         <span class="n">fid</span> <span class="o">=</span> <span class="n">amp</span> <span class="o">*</span> <span class="n">fid_sim</span><span class="p">(</span><span class="n">w0</span><span class="p">,</span> <span class="n">timepts</span><span class="p">,</span> <span class="n">t2</span><span class="p">,</span> <span class="n">phase_shift</span><span class="o">=</span><span class="n">phi</span><span class="p">)</span>
         <span class="n">fid_table</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fid</span><span class="p">)</span>

   <span class="c1"># Cast the list to an ndarray</span>
   <span class="n">fid_table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">fid_table</span><span class="p">)</span>

   <span class="c1"># Add some Gaussian noise to better simulate a real signal</span>
   <span class="n">noise</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="o">-</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.05</span><span class="p">,</span> <span class="n">fid_table</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
   <span class="n">noisy_fid</span> <span class="o">=</span> <span class="n">fid_table</span> <span class="o">+</span> <span class="n">noise</span>

   <span class="c1"># Shuffle the generated FIDs</span>
   <span class="n">rng</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">noisy_fid</span><span class="p">)</span>

   <span class="k">return</span> <span class="n">noisy_fid</span>
</pre></div>
</div>
</div>
</div>
<p>The following cell contains the simulated FIDs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">full_fid_table</span> <span class="o">=</span> <span class="n">make_fid_table</span><span class="p">(</span><span class="n">rf_mix_frequency</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">spin2_relaxation</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Table of simulated FIDs to serve as observed NMR spectra.</span>
<span class="n">full_fid_table</span> <span class="o">=</span> <span class="n">make_fid_table</span><span class="p">(</span><span class="n">rf_mix_frequency</span><span class="p">,</span> <span class="n">time</span><span class="p">,</span> <span class="n">spin2_relaxation</span><span class="p">)</span>

<span class="n">acq_fid_num</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="mf">0.95</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">full_fid_table</span><span class="p">)))</span>

<span class="n">observed_fid_table</span> <span class="o">=</span> <span class="n">full_fid_table</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="n">acq_fid_num</span><span class="p">]</span>
<span class="n">test_fid_table</span> <span class="o">=</span> <span class="n">full_fid_table</span><span class="p">[</span><span class="n">acq_fid_num</span><span class="p">:]</span>
</pre></div>
</div>
</div>
</div>
<p>The next step is to compute the Fourier transform of the FIDs that have just been generated.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">make_window</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Windows the input array relative to a signal&#39;s upper Nyquist band. &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">arr</span><span class="p">[:</span><span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span><span class="o">/</span><span class="mi">2</span><span class="p">))]</span>

<span class="k">def</span> <span class="nf">transform_fid_table</span><span class="p">(</span><span class="n">fid_table</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Computes the forward FFT of the input FID array and returns the upper Nyquist band. &quot;&quot;&quot;</span>
    <span class="c1"># Use a list for efficient append</span>
    <span class="n">fid_transform_table</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># Iterate over each FID in the FID table</span>
    <span class="k">for</span> <span class="n">fid</span> <span class="ow">in</span> <span class="n">fid_table</span><span class="p">:</span>
        <span class="c1"># Computes forward FFT and scales with the FID length</span>
        <span class="n">fid_ft</span>      <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fft</span><span class="p">(</span><span class="n">fid</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">fid</span><span class="p">)</span>
        <span class="c1"># Collects complex modulus of the frequency-domain signal</span>
        <span class="n">abs_ft</span>      <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">fid_ft</span><span class="p">)</span>
        <span class="c1"># Windows the FFT output to the upper Nyquist band</span>
        <span class="n">win_fid_ft</span>  <span class="o">=</span> <span class="n">make_window</span><span class="p">(</span><span class="n">abs_ft</span><span class="p">)</span>

        <span class="n">fid_transform_table</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">win_fid_ft</span><span class="p">)</span>

    <span class="c1"># Cast the list to an ndarray</span>
    <span class="n">transformed_fid_table</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">fid_transform_table</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">transformed_fid_table</span>
</pre></div>
</div>
</div>
</div>
<p>The following cell contains the Fourier transforms of the generated FIDs.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">ft_table</span> <span class="o">=</span> <span class="n">transform_fid_table</span><span class="p">(</span><span class="n">observed_fid_table</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The following cell is the frequency range of the signals, respective to the minimum sampling rate dictated by the highest frequency with which the FIDs were generated.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Frequency range based on Nyquist limit</span>
<span class="n">xfrq</span> <span class="o">=</span> <span class="n">make_window</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fftfreq</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">time</span><span class="p">),</span> <span class="n">d</span><span class="o">=</span><span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">2</span><span class="o">*</span><span class="n">min_sampling_freq</span><span class="p">)))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="the-neural-network">
<h2>The Neural Network<a class="headerlink" href="#the-neural-network" title="Permalink to this headline">#</a></h2>
<p>Can a neural network learn to decipher the frequency content of an observed FID? The first attempt will make use of simple artificial neural network (ANN) architecture trained on the FIDs and their respective FFT. This neural network consists of sequential dense, or fully-connected, layers which forms the <em>model</em>. The dense layers have several tunable parameters, otherwise known as <em>hyperparameters</em>. The <code class="docutils literal notranslate"><span class="pre">Dense</span></code> layer is defined in Keras as</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
    <span class="n">units</span><span class="p">,</span>
    <span class="n">activation</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">use_bias</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">kernel_initializer</span> <span class="o">=</span> <span class="s2">&quot;glorot_uniform&quot;</span><span class="p">,</span>
    <span class="n">bias_initializer</span> <span class="o">=</span> <span class="s2">&quot;zeros&quot;</span><span class="p">,</span>
    <span class="n">kernel_regularizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">bias_regularizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">activity_regularizer</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">kernel_constraint</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">bias_constraint</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>
</div>
<p>where the arguments provided represent the default values instantiated by Keras.</p>
<p>Some of the most commonly tuned hyperparameters are <code class="docutils literal notranslate"><span class="pre">units</span></code>, which corresponds to the output dimensions of the layer (i.e. the number of <em>nodes</em>), and <code class="docutils literal notranslate"><span class="pre">activation</span></code> to specify which activation function should be used. The number of layers in the model is also a tunable hyperparameter.</p>
<p>An important note about hyperparameters is that they are not universally constant - they require tuning by the user. This means that a network will potentially undergo many iterations before the quality of its predictions is satisfactory to the user.</p>
<p>The largest hurdle in developing a neural network is understanding what the network is supposed to be solving. In the forward direction, the FFT is a linear transformation between the time-domain and frequency-domain representations of a signal. Thus, it behaves more like a regression than a classification. This immediately limits our choice of activation function, making the <code class="docutils literal notranslate"><span class="pre">activation</span></code> hyperparameter simple to tune.</p>
<p>Keras maintains several activation functions. Some of the most popular are <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code>, <code class="docutils literal notranslate"><span class="pre">softmax</span></code>, <code class="docutils literal notranslate"><span class="pre">relu</span></code>, and <code class="docutils literal notranslate"><span class="pre">tanh</span></code>. Functions such as <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> and <code class="docutils literal notranslate"><span class="pre">softmax</span></code> are more useful in the final layer of network classifers, since they normalize the output to a probability distribution. That means <code class="docutils literal notranslate"><span class="pre">relu</span></code>-type and <code class="docutils literal notranslate"><span class="pre">linear</span></code> activation functions are preferred for regression problems, since they <em>generally</em> allow the output to remain unconstrained between <span class="math notranslate nohighlight">\(-\infty\)</span> and <span class="math notranslate nohighlight">\(\infty\)</span>.</p>
<p>An exaustive list of Keras’s activation functions and short descriptions of each may be found in the <a class="reference external" href="https://keras.io/api/layers/activations/#available-activations">Keras Activation Function API</a> documentation.</p>
<section id="exercise-1">
<h3>Exercise 1<a class="headerlink" href="#exercise-1" title="Permalink to this headline">#</a></h3>
<p>The ANN below is able to learn an FID’s frequency content, but it has trouble learning the amplitude and spectral-width of the center frequency. Adjust the parameters shown in the <code class="docutils literal notranslate"><span class="pre">Dense</span></code> API listed above.</p>
<ul class="simple">
<li><p>Focus primarily on layer <code class="docutils literal notranslate"><span class="pre">units</span></code> and activation functions.</p></li>
<li><p>Leave the initializers as default - these specify how the weights and biases are initialized in the model.</p></li>
</ul>
<section id="layers">
<h4><strong>Layers</strong><a class="headerlink" href="#layers" title="Permalink to this headline">#</a></h4>
<p>A neural network is expressed as a <em>model</em> composed of <em>layers</em>. The first layer in a network is the <em>input layer</em>, and the last layer is the <em>output layer</em>. The layers in between are the <em>hidden layers</em>, and is where the network’s “learning” occurs. Each layer is comprised of <em>nodes</em>.</p>
<p>Nodes simply hold a number. For the input layer, this number is the input feature itself; for instance, it could be the intensity of a pixel in an image, or a value in a vector of data. This number is also known as the <em>activation</em> - called <span class="math notranslate nohighlight">\(h_{n}^{(k)}\)</span> henceforth, where <span class="math notranslate nohighlight">\(n\)</span> is the index of the node in a layer of <span class="math notranslate nohighlight">\(N\)</span> nodes (or features), and <span class="math notranslate nohighlight">\(k\)</span> is the index of the layer in which that node resides. The network learns parameters called weights, <span class="math notranslate nohighlight">\(w_{n}\)</span>, which are used to weight the activation of a node in the previous layer and feed it forward to the next layer. The weights help the network determine which features are most important. In the <code class="docutils literal notranslate"><span class="pre">Dense</span></code> layers used in this network, each node is fully connected to the nodes of the next layer. Thus, each node in some layer <span class="math notranslate nohighlight">\(k\)</span> after the input contains a value expressed by</p>
<p>\begin{equation}
h_{n}^{(k+1)} = \sum_{n=1}^{N},h_{n}^{(k)}w_{n,k}
\end{equation}</p>
<p>Depending on the data on which the network is learning, there may be additional requirements to satisfy activation of a node, such as a numeric threshold. A <em>bias</em> term may be added to the weighted sum describing a node’s activation, such that the activation expression for a node may now be expressed as</p>
<p>\begin{equation}
h_{n}^{(k+1)} = b_n + \sum_{n=1}^{N},h_{n}^{(k)}w_{n,k}
\end{equation}</p>
<p>in which <span class="math notranslate nohighlight">\(b_n\)</span> shifts the value of the weighted sum to account for some bias. For example, if the threshold for activation is desired to be <span class="math notranslate nohighlight">\(h_{n}^{(k)} &gt; 5\)</span>, the bias term would be <span class="math notranslate nohighlight">\(b_n = -5\)</span>.</p>
<p>Typically, the number of nodes in the input layer corresponds to the number of features in the input data, and the number of nodes in the output layer is equal to the number of solutions to the problem being solved. For example, in a network classifying whether an animal is a dog or a cat, the input layer may be the dimensions of 3 features - length, weight, and fur color - and the output layer may consist of 2 nodes, 1 for “dog” and 1 for “cat”:</p>
<p><img alt="Network Diagram" src="https://github.com/GDS-Education-Community-of-Practice/DSECOP/blob/main/NMR_Deep_Learning/Resources/network2.png?raw=true" /></p>
<p>This is an example of a network with 3 inputs, <span class="math notranslate nohighlight">\(x_n\)</span>, and a single hidden layer consisting of 5 nodes. The lines between the nodes are colored blue for negative weights and orange for positive weights. There may be any number of hidden layers, and the dimensions of those layers may be tuned to fit the problem being solved, such as two dimensions for images (e.g. <code class="docutils literal notranslate"><span class="pre">128x128</span></code> for a picture) and three dimensions for time-series data.</p>
<p>The number of layers is an example of a <em>hyperparameter</em>, which are network parameters that must be iteratively tuned and generally cannot be learned by the model. This is in contrast to <em>parameters</em>, which are learned by the network, e.g. weights. The number of layers in a model is a readily tunable hyperparameter. The number of nodes in a layer is also a tunable hyperparameter. These hyperparameters represent the foundation of the model. The following ANN consists of a sequence of <code class="docutils literal notranslate"><span class="pre">Dense</span></code> layers, for the general rule is that more layers are better - but not always (e.g. overfitting).<span class="math notranslate nohighlight">\(^1\)</span></p>
<hr class="docutils" />
<p><strong>Exercise 1.1</strong> Try adding and removing <code class="docutils literal notranslate"><span class="pre">Dense</span></code> layers with varying numbers of nodes. Do any trends regarding the number of nodes and layers stick out? The <a class="reference external" href="https://keras.io/api/layers/core_layers/dense/">Keras Dense Layer API</a> lists the arguments necessary for instantiating a <code class="docutils literal notranslate"><span class="pre">Dense</span></code> layer.</p>
</section>
<section id="activation-functions">
<h4><strong>Activation Functions</strong><a class="headerlink" href="#activation-functions" title="Permalink to this headline">#</a></h4>
<p>Activation functions are some of the most important components of a network. These functions may be linear or nonlinear; the nonlinear functions are what allow the network to learn complex relationships within the training data. Activation functions are also responsible for constraining the data within physically meaningful bounds. As a simple example, classification networks tend to be concerned with percentages describing the confidence at which a network can classify an input as belonging to a particular group. This means that the activations <span class="math notranslate nohighlight">\(h_{n}^{(k)}\)</span> should ideally be constrained on the interval <span class="math notranslate nohighlight">\([0,1]\)</span>. For a regression problem, the boundaries could be defined on any interval meaningful to the data. Thus, different activation functions are better suited for different problems, and using the wrong activation function can yield poor or meaningless output from the network. With the inclusion of the activation function, the node’s activation may now be expressed as</p>
<p>\begin{equation}
h_{n}^{(k+1)} = f\left(b + \sum_{n=1}^{N},h_{n}^{(k)}w_{n,k}\right)
\end{equation}</p>
<p>where the activation <span class="math notranslate nohighlight">\(h_{n}^{(k)}\)</span> is the argument of the activation function <span class="math notranslate nohighlight">\(f\)</span>.</p>
<section id="linear-function">
<h5><strong>Linear Function</strong><a class="headerlink" href="#linear-function" title="Permalink to this headline">#</a></h5>
<p>The <code class="docutils literal notranslate"><span class="pre">linear</span></code> activation function is simply</p>
<p>\begin{equation}
f(x)=x
\end{equation}</p>
<p>which means the output is unchanged from the input. In Keras’s <code class="docutils literal notranslate"><span class="pre">Dense</span></code> layers, <code class="docutils literal notranslate"><span class="pre">activation=None</span></code> is the default value, implying the default activation function is <code class="docutils literal notranslate"><span class="pre">linear</span></code>. This is also sometimes used in the output layer of regression networks since the function is unconstrained, e.g. if the solutions are valid on the interval <span class="math notranslate nohighlight">\((-\infty, \infty)\)</span>. Using the <code class="docutils literal notranslate"><span class="pre">linear</span></code> function in hidden layers prevents the network from learning more complex relationships about the input features, and thus is not recommended.</p>
</section>
<section id="sigmoid-function">
<h5><strong>Sigmoid Function</strong><a class="headerlink" href="#sigmoid-function" title="Permalink to this headline">#</a></h5>
<p>The <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> function clamps values on the interval <span class="math notranslate nohighlight">\([0,1]\)</span>, and has the form</p>
<p>\begin{equation}
\sigma{(z)}=\frac{1}{1+e^{-z}}
\end{equation}</p>
<p>which results in lower activation values being more heavily weighted towards zero, and higher activation towards one. This effectively produces a vector of probabilities. The <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> function is typically only used in the output layer of binary classification networks, since the resultant probabilities are not related and their sum is not contrained to be less than or equal to one.</p>
</section>
<section id="softmax-function">
<h5><strong>Softmax Function</strong><a class="headerlink" href="#softmax-function" title="Permalink to this headline">#</a></h5>
<p>A similar function, <code class="docutils literal notranslate"><span class="pre">softmax</span></code>, is a generalizion of the <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code> function and is used for higher order classification involving more than 2 groups. It is defined as</p>
<p>\begin{equation}
\sigma{(\mathbf{z})}<em>{j}=\frac{e^{z</em>{j}}}{\sum_{c=1}^{C}e^{z_c}}
\end{equation}</p>
<p>where <span class="math notranslate nohighlight">\(C\)</span> is the number of distinct groups, and <span class="math notranslate nohighlight">\(j\)</span> is on the interval <span class="math notranslate nohighlight">\([1,C]\)</span>. This function is used in the output layer for multi-class classification (<span class="math notranslate nohighlight">\(C&gt;2\)</span>) since the resultant probabilities are related and sum to one.</p>
</section>
<section id="rectified-linear-unit-relu-function">
<h5><strong>Rectified Linear Unit (ReLU) Function</strong><a class="headerlink" href="#rectified-linear-unit-relu-function" title="Permalink to this headline">#</a></h5>
<p>The <code class="docutils literal notranslate"><span class="pre">ReLU</span></code> function is frequently used as the activation function for hidden layers, regardless of whether the network classifies or regresses. It has the form</p>
<p>\begin{equation}
f(z)=max(z,0)
\end{equation}</p>
<p>which simply clamps the input to values on the interval <span class="math notranslate nohighlight">\([0,\infty)\)</span>. Thus, negative activations are rendered as zero, and positive activations are left unchanged. This function may also be used in the output later if the solutions are desired to be positive. The <code class="docutils literal notranslate"><span class="pre">ReLU</span></code> function and other similar functions, e.g. <code class="docutils literal notranslate"><span class="pre">ReLU6</span></code>, <code class="docutils literal notranslate"><span class="pre">SELU</span></code>, <code class="docutils literal notranslate"><span class="pre">ELU</span></code>, and <code class="docutils literal notranslate"><span class="pre">Swish</span></code>, are some of the most frequently used activation functions when designing neural networks.</p>
<p>Depending on the requirements of the network, it is also possible to design a custom activation function. Activation functions are most often non-linear and differentiable. Non-linearity allows the network to parse out more complex relationships within the data. If the activation function is differentiable, this allows for its gradient to be computed, which facilitates optimization methods such as <em>Stochastic Gradient Descent</em> (SGD).</p>
<hr class="docutils" />
<p><strong>Exercise 1.2</strong> Try different activation functions in one, some, or all layers; however, limit the functions to <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code>, <code class="docutils literal notranslate"><span class="pre">softmax</span></code>, <code class="docutils literal notranslate"><span class="pre">relu</span></code>, and <code class="docutils literal notranslate"><span class="pre">linear</span></code>. A linear activation function is the default value of a Keras <code class="docutils literal notranslate"><span class="pre">Dense</span></code> layer, specified by <code class="docutils literal notranslate"><span class="pre">activation</span> <span class="pre">=</span> <span class="pre">None</span></code> What differences in behavior arise when using different activation functions? Arguments for the activation functions may be found in the <a class="reference external" href="https://keras.io/api/layers/activations/">Keras Activation Function API</a> documentation.</p>
</section>
</section>
<section id="regularization">
<h4><strong>Regularization</strong><a class="headerlink" href="#regularization" title="Permalink to this headline">#</a></h4>
<p>Regularization is a method used to prevent overfitting in the model. A model ultimately seeks to fit its learned parameters to the training data in order to produce a generalizable solution that works on unseen data. If a model is underfit, this means that the model has trouble converging to a solution for the training data. If the model is overfit, this means that the model performs accurately on the training data, but the solution is poorly generalized; thus, unseen data fed to the network will yield poor results.</p>
<section id="l1-and-l2-regularizers">
<h5><strong>L1 and L2 Regularizers</strong><a class="headerlink" href="#l1-and-l2-regularizers" title="Permalink to this headline">#</a></h5>
<p>A model seeks to converge by minimizing a loss function. A popular loss function is <em>mean squared error</em> (MSE), which has the general form</p>
<p>\begin{equation}
\text{MSE} = \frac{1}{M}\sum_{m=1}^{M}\left(y_m - \hat{y}_{m} \right)^2
\end{equation}</p>
<p>where <span class="math notranslate nohighlight">\(M\)</span> is the number of samples, <span class="math notranslate nohighlight">\(y_m\)</span> is some known, actual value (a.k.a. <em>ground truth</em>), and <span class="math notranslate nohighlight">\(\hat{y}_{m}\)</span> is the predicted value. In the context of this module’s neural network, this may have the form</p>
<p>\begin{equation}
\text{MSE} = \frac{1}{M}\sum_{m=1}^{M}\left(y_m - f\left(b_n + \sum_{n=1}^{N},h_{n}^{(k)}w_{n,k}\right) \right)^2
\end{equation}</p>
<p>The optimizer will work to minimize the MSE in order to determine the quality of the solution upon which it has converged, i.e. good fitness. A clear indicator of overfitting is when the training loss, denoted as <code class="docutils literal notranslate"><span class="pre">loss</span></code> in Keras, converges much more rapidly than the validation loss, denoted by Keras as <code class="docutils literal notranslate"><span class="pre">val_loss</span></code>. Another indicator is when <code class="docutils literal notranslate"><span class="pre">val_loss</span></code> converges early, such that <code class="docutils literal notranslate"><span class="pre">loss</span></code> <span class="math notranslate nohighlight">\(\ll\)</span> <code class="docutils literal notranslate"><span class="pre">val_loss</span></code>. This means that the model is poorly generalized and is failing to converge well on the validation data.</p>
<p>Regularization attempts to reduce MSE by punishing large weights with the introduction of a penalty term. Two of the most popular regularization techniques are <code class="docutils literal notranslate"><span class="pre">L1</span></code> and <code class="docutils literal notranslate"><span class="pre">L2</span></code>, where the penalty term is simply added to the MSE. Thus, the regularized MSE can be written in the form</p>
<p>\begin{equation}
\begin{aligned}
\text{MSE}<em>{L1} &amp;= \frac{1}{M}\sum</em>{m=1}^{M}\left(y_m - \hat{y}<em>{m} \right)^2 + \lambda \sum</em>{n=1}^{N}\left|w_{n,k}\right|
\
\
\text{MSE}<em>{L2} &amp;= \frac{1}{M}\sum</em>{m=1}^{M}\left(y_m - \hat{y}<em>{m} \right)^2 + \underbrace{\lambda \sum</em>{n=1}^{N}w_{n,k}^2}_{\text{Penalty}}\
\end{aligned}
\end{equation}</p>
<p>The hyperparameter <span class="math notranslate nohighlight">\(\lambda\)</span> is typically a positive number. A larger <span class="math notranslate nohighlight">\(\lambda\)</span> increases the MSE, especially so for large <span class="math notranslate nohighlight">\(w\)</span>, which suggests that the model should continue to optimize. A smaller <span class="math notranslate nohighlight">\(\lambda\)</span> shrinks the MSE, doing the opposite. The penalty <span class="math notranslate nohighlight">\(\lambda\)</span> often requires iterations to the tuning in order to determine a good value for this hyperparameter.</p>
</section>
<section id="dropout">
<h5><strong>Dropout</strong><a class="headerlink" href="#dropout" title="Permalink to this headline">#</a></h5>
<p>Another regularization method is <em>dropout</em>, which may be implemented via a Keras <code class="docutils literal notranslate"><span class="pre">Dropout</span></code> layer as</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>  <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">rate</span><span class="p">)</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">rate</span></code> is a value between 0 and 1 to indicate how much of the data should drop out. Dropout essentially introduces noise to the data by removing nodes, thus reconfiguring the connectivity of nodes in subsequent layers.</p>
</section>
<section id="early-stopping">
<h5><strong>Early Stopping</strong><a class="headerlink" href="#early-stopping" title="Permalink to this headline">#</a></h5>
<p><em>Early stopping</em> is a useful regularization technique that halts model training when the <code class="docutils literal notranslate"><span class="pre">loss</span></code> or <code class="docutils literal notranslate"><span class="pre">val_loss</span></code> (or other metric, depending on the model) has reached a minimum. This helps prevent the optimizer from deviating from a found minumum, and is implemented as a callback in the <code class="docutils literal notranslate"><span class="pre">model.fit</span></code> method.</p>
<hr class="docutils" />
<p><strong>Exercise 1.3</strong> Try tuning the <code class="docutils literal notranslate"><span class="pre">L1</span></code> and/or <code class="docutils literal notranslate"><span class="pre">L2</span></code> regularizers at the kernel level on one, some, or all layers to see how this affects the neural network’s ability to learn. This can be implemented in the <code class="docutils literal notranslate"><span class="pre">Dense</span></code> layer via</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>  <span class="n">kernel_regularizer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">L1</span><span class="p">(</span><span class="n">l1</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
  <span class="n">kernel_regularizer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">L2</span><span class="p">(</span><span class="n">l2</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
<p>Note that the <code class="docutils literal notranslate"><span class="pre">kernel_regularizer</span></code> will attempt to penalize large weights without considering the bias. The <code class="docutils literal notranslate"><span class="pre">activity_regularizer</span></code> attempts to reduce the output of a layer by accounting for both the weights and the bias. The <code class="docutils literal notranslate"><span class="pre">bias_regularizer</span></code> reduces the bias. For layers with no bias, <code class="docutils literal notranslate"><span class="pre">kernel_regularizer</span></code> is sufficient.</p>
<p>Further information about layer arguments may be found in the <a class="reference external" href="https://keras.io/api/layers/core_layers/">Keras Layers API</a> documentation.</p>
</section>
</section>
<section id="optimizer">
<h4><strong>Optimizer</strong><a class="headerlink" href="#optimizer" title="Permalink to this headline">#</a></h4>
<section id="stochastic-gradient-descent">
<h5><strong>Stochastic Gradient Descent</strong><a class="headerlink" href="#stochastic-gradient-descent" title="Permalink to this headline">#</a></h5>
<p>The optimizer is the function that minimizes the loss function. The optimizer used in this network is <em>Stochastic Gradient Descent</em> (SGD), and the loss function is the <em>mean squared error</em> (MSE). New weights are determined via SGD in this module’s network for the next iteration of learning by</p>
<p>\begin{equation}
w_{n,k}^{(new)} = w_{n,k}^{(old)} - \alpha \frac{\partial{\text{MSE}}}{\partial{,w_{n,k}^{(old)}}}
\end{equation}</p>
<p>where <span class="math notranslate nohighlight">\(w_{n,k}^{(new)}\)</span> is the updated weight, <span class="math notranslate nohighlight">\(w_{n,k}^{(old)}\)</span> is the current weight, and <span class="math notranslate nohighlight">\(\alpha\)</span> is the <em>learning rate</em>. This method, however, differs from traditional gradient descent isn that SGD optimizes via randomly chosen samples, whereas gradient descent utilizes <em>all</em> samples. Thus, SGD may significantly improve optimization time.</p>
</section>
<section id="learning-rate">
<h5><strong>Learning Rate</strong><a class="headerlink" href="#learning-rate" title="Permalink to this headline">#</a></h5>
<p>The learning rate <span class="math notranslate nohighlight">\(\alpha\)</span> is a hyperparameter that controls how quickly the optimizer adjusts weights. Imagining the convergence as traversal of a parabola, the learning rate affects how quickly the bottom of the parabola is reached; a smaller <span class="math notranslate nohighlight">\(\alpha\)</span> means the optimizer converges more slowly, and a larger <span class="math notranslate nohighlight">\(\alpha\)</span> means faster convergence. It would seem that a larger <span class="math notranslate nohighlight">\(\alpha\)</span> would be ideal, but if there exist multiple minima in the optimization, a faster learning rate could overshoot. A lower learning rate reduces the risk of overshoot, but may take longer to converge. As with other hyperparameters, this may require tuning to reach a suitable value.</p>
<p>Learning rates in Keras may be implemented as a schedule, which affords finer control of the learning rate by introducing decay, which allows the model to begin training with a high learning rate which decays as training progresses. This offers a compromise between convergence speed and accuracy.</p>
</section>
<section id="momentum">
<h5><strong>Momentum</strong><a class="headerlink" href="#momentum" title="Permalink to this headline">#</a></h5>
<p>An important optimization hyperparameter is the <em>momentum</em>. The momentum works much in the same way as physical inertia - as the optimizer converges on a solution, momentum allows it to retain some degree of the previous optimization’s direction as it searches for minima, potentially decreasing convergence time and allowing for better traversal across local minima.</p>
<hr class="docutils" />
<p>SGD finds the derivatives of the loss function in order to find local minima. This method is susceptible to vanishing and exploding gradients, which result in weights being calculated as <code class="docutils literal notranslate"><span class="pre">nan</span></code>. The <code class="docutils literal notranslate"><span class="pre">clipnorm</span></code> argument in the optimizer clips exploding gradients at <code class="docutils literal notranslate"><span class="pre">1.0</span></code>.</p>
<p><strong>Exercise 1.4</strong> Try different optimizers and try tuning the optimization hyperparameters. From the descriptions of the optimizers, why might some be poorly suited to this problem? The possible optimizers for use with Keras models are listed in the <a class="reference external" href="https://keras.io/api/optimizers/#available-optimizers">Keras Optimizers API</a> documentation.</p>
<p><strong>Excercise 1.5</strong> Tune the hyperparameters in <code class="docutils literal notranslate"><span class="pre">lr_schedule</span></code> and observe the effect on <code class="docutils literal notranslate"><span class="pre">loss</span></code> and <code class="docutils literal notranslate"><span class="pre">val_loss</span></code>. Learning rate schedules and API usage are listed in the <a class="reference external" href="https://keras.io/api/optimizers/learning_rate_schedules/">Keras Learning Schedule API</a> documentation.</p>
</section>
</section>
</section>
<section id="references">
<h3>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h3>
<hr class="docutils" />
<p>[1] <a class="reference external" href="https://arxiv.org/abs/1206.5533">Bengio, Yoshua. “Practical recommendations for gradient-based training of deep architectures.” Neural networks: Tricks of the trade. Springer Berlin Heidelberg, 2012. 437-478.</a></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_ft</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="c1"># Input layer</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span>
        <span class="n">shape</span> <span class="o">=</span> <span class="n">observed_fid_table</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="p">),</span>
    <span class="c1"># Hidden layer 1</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="n">activation</span>  <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
        <span class="n">units</span>       <span class="o">=</span> <span class="n">observed_fid_table</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">use_bias</span>    <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
        <span class="n">kernel_regularizer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">regularizers</span><span class="o">.</span><span class="n">L1</span><span class="p">(</span>
            <span class="n">l1</span> <span class="o">=</span> <span class="mf">1e-4</span>
        <span class="p">)</span>
    <span class="p">),</span>
    <span class="c1"># Output layer</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="c1"># Linear output layer - regression-type problem</span>
        <span class="n">units</span>       <span class="o">=</span> <span class="n">ft_table</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">use_bias</span>    <span class="o">=</span> <span class="kc">False</span>
    <span class="p">)</span>
<span class="p">])</span>

<span class="c1"># Callback to stop the model early based on `val_loss` to prevent overfitting</span>
<span class="n">early_stop_callback</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span>
    <span class="n">monitor</span>   <span class="o">=</span> <span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> 
    <span class="n">min_delta</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span> 
    <span class="n">patience</span>  <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> 
    <span class="n">mode</span>      <span class="o">=</span> <span class="s1">&#39;auto&#39;</span><span class="p">,</span>
    <span class="n">verbose</span>   <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">baseline</span>  <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span>

<span class="c1"># Learning rate schedule to decrease the learning rate over the training period</span>
<span class="n">lr_schedule</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">schedules</span><span class="o">.</span><span class="n">ExponentialDecay</span><span class="p">(</span>
    <span class="n">initial_learning_rate</span>   <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">decay_steps</span>             <span class="o">=</span> <span class="mi">500</span><span class="p">,</span>
    <span class="n">decay_rate</span>              <span class="o">=</span> <span class="mf">0.96</span>
<span class="p">)</span>

<span class="n">model_ft</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span>        <span class="o">=</span> <span class="s1">&#39;mean_squared_error&#39;</span><span class="p">,</span> 
    <span class="n">optimizer</span>   <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span>
        <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">lr_schedule</span><span class="p">,</span>
        <span class="n">momentum</span>      <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="c1"># Prevents vanishing/exploding gradients</span>
        <span class="n">clipnorm</span>      <span class="o">=</span> <span class="mf">1.0</span>
        <span class="p">)</span>
    <span class="p">)</span>

<span class="n">model_ft</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">x</span>                 <span class="o">=</span> <span class="n">observed_fid_table</span><span class="p">,</span> 
    <span class="n">y</span>                 <span class="o">=</span> <span class="n">ft_table</span><span class="p">,</span> 
    <span class="n">epochs</span>            <span class="o">=</span> <span class="mi">300</span><span class="p">,</span>
    <span class="n">batch_size</span>        <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="c1"># Train on 80% of the input data, validate on 20%</span>
    <span class="n">validation_split</span>  <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">verbose</span>           <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">callbacks</span>         <span class="o">=</span> <span class="p">[</span><span class="n">early_stop_callback</span><span class="p">]</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>A summary of the neural network may be seen below. The number of parameters is exceptionally large. When working with unmixed frequencies in the MHz regime, the Nyquist limit would rapidly drive the number of parameters beyond the point of overflow.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_ft</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense (Dense)               (None, 4000)              16000000  
                                                                 
 dense_1 (Dense)             (None, 2000)              8000000   
                                                                 
=================================================================
Total params: 24,000,000
Trainable params: 24,000,000
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
</section>
<section id="ann-predictions">
<h3>ANN Predictions<a class="headerlink" href="#ann-predictions" title="Permalink to this headline">#</a></h3>
<p>An FID may be submitted to the model via the <code class="docutils literal notranslate"><span class="pre">predict()</span></code> method, which will return the neural network’s estimation of the FID’s frequency spectrum.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">nn_fft</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
    <span class="n">prediction</span> <span class="o">=</span> <span class="n">model_ft</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">prediction</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title ANN Predictions Visualized</span>
<span class="c1">#@markdown Press the `Play` button on the left side of the cell to visualize the ANN predictions for a randomly generated FID. On the left is the submitted FID, and on the right is a superimposition of the ANN prediction and the real FFT output for that FID.</span>
<span class="c1">#@markdown The frequency in the ANN Prediction callout is the center frequency predicted by the network. Check the results of hyperparameter tuning against the output in this figure.</span>
<span class="c1">#@markdown</span>
<span class="c1">#@markdown ---</span>
<span class="c1">#@markdown **Exercise 1.1.6** Run this code several times and observe how the network&#39;s estimated resonant frequency compares to the real frequency from the FFT.</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">test_fid_table</span><span class="p">)</span>
<span class="n">rand_idx</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="n">low</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="n">n_samples</span><span class="p">)</span>
<span class="n">rand_sample</span> <span class="o">=</span> <span class="n">test_fid_table</span><span class="p">[</span><span class="n">rand_idx</span><span class="p">]</span>

<span class="n">nn_test</span> <span class="o">=</span> <span class="n">nn_fft</span><span class="p">(</span><span class="n">rand_sample</span><span class="p">)</span>
<span class="n">ft_real</span> <span class="o">=</span> <span class="n">make_window</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fft</span><span class="p">(</span><span class="n">rand_sample</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">rand_sample</span><span class="p">)))</span>

<span class="n">nn_peak_at</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">nn_test</span><span class="p">))</span>
<span class="n">ft_peak_at</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">ft_real</span><span class="p">)</span>
<span class="n">peak_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">nn_peak_at</span><span class="p">,</span> <span class="n">ft_peak_at</span><span class="p">])</span>

<span class="n">nn_freq</span> <span class="o">=</span> <span class="n">xfrq</span><span class="p">[</span><span class="n">nn_peak_at</span><span class="p">]</span>
<span class="n">ft_freq</span> <span class="o">=</span> <span class="n">xfrq</span><span class="p">[</span><span class="n">ft_peak_at</span><span class="p">]</span>

<span class="c1">#*_, axis = plt.subplots(1, 3, figsize = (24, 5))</span>
<span class="o">*</span><span class="n">_</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span> <span class="o">=</span> <span class="p">(</span><span class="mi">17</span><span class="p">,</span> <span class="mi">5</span><span class="p">))</span>
<span class="n">sb</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">time</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">rand_sample</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axis</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;FID&#39;</span><span class="p">)</span>
<span class="n">sb</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">xfrq</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">nn_test</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axis</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;ANN Prediction&#39;</span><span class="p">)</span>
<span class="n">sb</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">xfrq</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">ft_real</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">axis</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;FFT Output&#39;</span><span class="p">)</span>

<span class="n">axis</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;Observed FID&quot;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Time (s)&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Amplitude&quot;</span><span class="p">)</span>
<span class="n">axis</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;FID Frequency Spectrum vs ANN Predicted Spectrum&quot;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Frequency (Hz)&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Amplitude&quot;</span><span class="p">)</span>

<span class="c1"># Annotations</span>
<span class="n">arrow_args</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">arrowstyle</span><span class="o">=</span><span class="s1">&#39;-&gt;&#39;</span><span class="p">)</span>
<span class="n">bbox_args</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s1">&#39;square&#39;</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">)</span>
<span class="n">bbox_args2</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">boxstyle</span><span class="o">=</span><span class="s1">&#39;round&#39;</span><span class="p">,</span> <span class="n">fc</span><span class="o">=</span><span class="s1">&#39;white&#39;</span><span class="p">,</span> <span class="n">edgecolor</span><span class="o">=</span><span class="s1">&#39;gray&#39;</span><span class="p">,</span> <span class="n">linewidth</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">)</span>

<span class="n">nn_amp</span> <span class="o">=</span> <span class="n">nn_test</span><span class="p">[</span><span class="n">nn_peak_at</span><span class="p">]</span>
<span class="n">ft_amp</span> <span class="o">=</span> <span class="n">ft_real</span><span class="p">[</span><span class="n">ft_peak_at</span><span class="p">]</span>

<span class="k">if</span> <span class="p">(</span><span class="n">nn_amp</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">):</span> 
  <span class="n">nn_y_annotate</span> <span class="o">=</span> <span class="n">nn_amp</span> <span class="o">-</span> <span class="mf">0.025</span>
<span class="k">else</span><span class="p">:</span>
  <span class="n">nn_y_annotate</span> <span class="o">=</span> <span class="n">nn_amp</span> <span class="o">+</span> <span class="mf">0.01</span>

<span class="n">axis</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">nn_freq</span><span class="si">}</span><span class="s1"> Hz&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">nn_freq</span><span class="p">,</span> <span class="n">nn_amp</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">nn_freq</span> <span class="o">+</span> <span class="mi">8</span><span class="p">,</span> <span class="n">nn_y_annotate</span><span class="p">),</span> <span class="n">arrowprops</span><span class="o">=</span><span class="n">arrow_args</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="n">bbox_args</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">axis</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">ft_freq</span><span class="si">}</span><span class="s1"> Hz&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="n">ft_freq</span><span class="p">,</span> <span class="n">ft_amp</span><span class="p">),</span> <span class="n">xytext</span><span class="o">=</span><span class="p">(</span><span class="n">ft_freq</span> <span class="o">-</span> <span class="mi">19</span><span class="p">,</span> <span class="n">ft_amp</span><span class="o">-</span><span class="mf">0.075</span><span class="p">),</span> <span class="n">arrowprops</span><span class="o">=</span><span class="n">arrow_args</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="n">bbox_args</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

<span class="n">first_legend</span> <span class="o">=</span> <span class="n">axis</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">axis</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">add_artist</span><span class="p">(</span><span class="n">first_legend</span><span class="p">)</span>

<span class="n">axis</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">annotate</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Δf=</span><span class="si">{</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">nn_freq</span><span class="o">-</span><span class="n">ft_freq</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span> <span class="n">xy</span><span class="o">=</span><span class="p">(</span><span class="mf">0.7425</span><span class="p">,</span> <span class="mf">0.795</span><span class="p">),</span> <span class="n">xycoords</span><span class="o">=</span><span class="s1">&#39;axes fraction&#39;</span><span class="p">,</span> <span class="n">bbox</span><span class="o">=</span><span class="n">bbox_args2</span><span class="p">,</span> <span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>

<span class="c1"># `np.isclose(a, b)` assumed b is the reference value</span>
<span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isclose</span><span class="p">(</span><span class="n">nn_peak_at</span><span class="p">,</span> <span class="n">ft_peak_at</span><span class="p">,</span> <span class="n">atol</span> <span class="o">=</span> <span class="mi">5</span><span class="p">):</span>
  <span class="n">axis</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">peak_mean</span> <span class="o">-</span> <span class="mi">50</span><span class="p">,</span> <span class="n">peak_mean</span> <span class="o">+</span> <span class="mi">50</span><span class="p">)</span>

<span class="k">if</span> <span class="p">(</span><span class="n">nn_amp</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">):</span>
  <span class="n">axis</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_ylim</span><span class="p">(</span><span class="n">nn_amp</span> <span class="o">-</span> <span class="mf">0.15</span><span class="p">,</span> <span class="n">ft_amp</span> <span class="o">+</span> <span class="mf">0.1</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/5a57cca8a675240ca96af9b77306646ff3bb49197a6cc67218e7042a34b63c77.png" src="../../_images/5a57cca8a675240ca96af9b77306646ff3bb49197a6cc67218e7042a34b63c77.png" />
</div>
</div>
</section>
<section id="discussion">
<h3>Discussion<a class="headerlink" href="#discussion" title="Permalink to this headline">#</a></h3>
<p>The neural network with the hyperparameters provided in this module was able to successfully learn the frequency content of the majority of FIDs submitted to the predictor. The network showed little capacity for learning the amplitude or spectral width of the center frequency. Some solutions to this problem are adding more layers or further tuning of the hyperparameters.</p>
<p>There exists an alternative to the approach above. Rather than training the network on the FID and its corresponding frequency spectrum, a network may be trained on the FID and the information one expects to discern from the spectrum.</p>
<p>This approach is advantageous because it significantly reduces the number of outputs, and subsequently the number of training parameters. The next exercise in this module will explore this approach in more depth.</p>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="exercise-2-estimation-of-lorentzian-curve-fitting-parameters-from-fid-frequency-spectra">
<h1><strong>Exercise 2</strong> Estimation of Lorentzian Curve Fitting Parameters from FID Frequency Spectra<a class="headerlink" href="#exercise-2-estimation-of-lorentzian-curve-fitting-parameters-from-fid-frequency-spectra" title="Permalink to this headline">#</a></h1>
<p>In the previous exercise, it was found that the ANN failed to produce meaningful estimates of the amplitude and spectral width of the FID’s center frequency. This information is important in spectroscopy. For instance, in <span class="math notranslate nohighlight">\(^1\)</span>H NMR, the area under the peak is proportional to the number of protons producing that peak. The sum of the peak integrals represents the number of hydrogens in a substance, which allows for estimation of a substance’s molecular formula. Thus, without accurate amplitude and width data, predicting the center frequency is functionally useless.</p>
<section id="curve-fitting-of-nmr-data">
<h2>Curve Fitting of NMR Data<a class="headerlink" href="#curve-fitting-of-nmr-data" title="Permalink to this headline">#</a></h2>
<p>Determining the location, amplitude, and spectral width of a spectrum’s resonant frequency is easily done by fitting the peak to a spectral line shape function. The most common line shapes encountered in NMR are <em>Lorentzian</em>, <em>Gaussian</em>, and <em>Voigt</em>.</p>
<p>\</p>
<section id="lorentzian-distribution">
<h3><strong>Lorentzian Distribution</strong><a class="headerlink" href="#lorentzian-distribution" title="Permalink to this headline">#</a></h3>
<p>The Lorentzian line shape function is given by its probability density function (PDF),</p>
<p>\begin{equation}
L(x; x_0, A, \gamma) = \frac{A}{\pi}\left(\frac{\gamma}{(x-x_0)^2 + \gamma^2}\right)
\end{equation}</p>
<p>where <span class="math notranslate nohighlight">\(x_0\)</span> is the location of the resonant frequency, <span class="math notranslate nohighlight">\(A\)</span> is the amplitude, and <span class="math notranslate nohighlight">\(\gamma\)</span> is the FWHM corresponding to the spectral width.</p>
<p>\</p>
</section>
<section id="gaussian-distribution">
<h3><strong>Gaussian Distribution</strong><a class="headerlink" href="#gaussian-distribution" title="Permalink to this headline">#</a></h3>
<p>Gaussian line shapes are similarly given by their PDF as</p>
<p>\begin{equation}
G(x; \mu, A, \sigma) = \frac{A}{\sigma\sqrt{2\pi}}\exp\left(-\frac{(x-\mu)^2)}{2\sigma^2}\right)
\end{equation}</p>
<p>where <span class="math notranslate nohighlight">\(\mu\)</span> corresponds to the location of the resonant frequency, <span class="math notranslate nohighlight">\(A\)</span> is again the amplitude, and <span class="math notranslate nohighlight">\(\sigma\)</span> is the FWHM describing the spectral width.</p>
<p>\</p>
</section>
<section id="voigt-profile">
<h3><strong>Voigt Profile</strong><a class="headerlink" href="#voigt-profile" title="Permalink to this headline">#</a></h3>
<p>The Voigt profile is a convolution of the Lorentzian and Gaussian line shapes, and is given by</p>
<p>\begin{equation}
V(x; \sigma, \gamma) = \int_{-\infty}^{\infty},G(x’;\sigma)L(x-x’;\gamma),dx’
\end{equation}</p>
<p>There is no closed-form expression for the Voigt profile, but may be approximated from the real part of the Faddeeva function <span class="math notranslate nohighlight">\(w(z)\)</span>,</p>
<p>\begin{equation}
V(x; \sigma, \gamma) = \frac{\Re\left[w(z)\right]}{\sigma\sqrt{2\pi}}, \qquad z=\frac{x+iy}{\sigma\sqrt{2}}
\end{equation}</p>
<p>Computing <span class="math notranslate nohighlight">\(w(z)\)</span> is computationally expensive, so the Voigt profile is most often approximated using a pseudo-Voigt profile,</p>
<p>\begin{equation}
pV(x; \sigma, \gamma) = \eta G(x;\sigma) + (1-\eta)L(x;\gamma)
\end{equation}</p>
<p>where <span class="math notranslate nohighlight">\(\eta\)</span> is a parameter bound between 0 and 1 that shifts the profile towards pure Gaussian or pure Lorentzian when approaching 1 or 0, respectively.</p>
</section>
<section id="lorentzian-fit">
<h3>Lorentzian Fit<a class="headerlink" href="#lorentzian-fit" title="Permalink to this headline">#</a></h3>
<p>The function below defines a Lorentzian line shape that is callable by <code class="docutils literal notranslate"><span class="pre">scipy.optimize.curve_fit()</span></code>. The parameters being estimated are the location of the resonant frequency, the amplitude of the peak, and the width of the peak.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">lorentzian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x0</span><span class="p">,</span> <span class="n">amp</span><span class="p">,</span> <span class="n">gamma</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Callable Lorentzian distribution for use with `scipy.optimize.curve_fit() &quot;&quot;&quot;</span>
    <span class="n">loc_param</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>     <span class="c1"># est. location param - peak location</span>
    <span class="n">scl_param</span> <span class="o">=</span> <span class="n">gamma</span><span class="o">**</span><span class="mi">2</span>        <span class="c1"># scale param specifying FWHM</span>

    <span class="c1"># Probability density function for Lorentzian distribution </span>
    <span class="n">lorentz_pdf</span> <span class="o">=</span> <span class="p">(</span><span class="n">amp</span><span class="o">/</span><span class="n">PI</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">gamma</span> <span class="o">/</span> <span class="p">(</span><span class="n">loc_param</span> <span class="o">+</span> <span class="n">scl_param</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">lorentz_pdf</span>
</pre></div>
</div>
</div>
</div>
<p>The curve fitting algorithm provided by <code class="docutils literal notranslate"><span class="pre">scipy</span></code> works best with initial guesses for the parameters. The peak locations and amplitudes may be robustly determined from the <code class="docutils literal notranslate"><span class="pre">scipy.signal.find_peaks()</span></code> peak finding algorithm by thresholding valid peaks to a peak prominence limited by the signal-to-noise ratio (SNR) of the frequency spectrum.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">fit_spectrum_lorentzian</span><span class="p">(</span><span class="n">ft_spectrum_table</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot; Fits the Fourier transform spectrum to a Lorentzian PDF for parameter estimation &quot;&quot;&quot;</span>
    <span class="n">popt_table</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># peak prominence threshold determined from SNR</span>
    <span class="n">snr_pdiff</span> <span class="o">=</span> <span class="mf">0.049</span>

    <span class="k">for</span> <span class="n">spectrum</span> <span class="ow">in</span> <span class="n">ft_spectrum_table</span><span class="p">:</span>
        <span class="c1"># Determine index of maximum amplitude</span>
        <span class="c1"># peaks = np.argmax(spectrum) # Less robust than peak finding</span>

        <span class="c1"># Determines peak based on prominence of peak relative to depth of side lobes</span>
        <span class="c1"># Adjust prominence until desired number of peaks are found</span>
        <span class="n">peaks</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">signal</span><span class="o">.</span><span class="n">find_peaks</span><span class="p">(</span>
            <span class="n">x</span>           <span class="o">=</span> <span class="n">spectrum</span><span class="p">,</span> 
            <span class="n">prominence</span>  <span class="o">=</span> <span class="n">snr_pdiff</span>
            <span class="p">)</span>
        <span class="n">amp_at_center_frq</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">peaks</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

        <span class="c1"># Estimate the the amplitude and location (x0) from the amp_at_center_frq</span>
        <span class="n">amp_pred</span> <span class="o">=</span> <span class="n">spectrum</span><span class="p">[</span><span class="n">amp_at_center_frq</span><span class="p">]</span>
        <span class="n">frq_pred</span> <span class="o">=</span> <span class="n">xfrq</span><span class="p">[</span><span class="n">amp_at_center_frq</span><span class="p">]</span>

        <span class="c1"># Return optimized parameters, discard covariance matrix</span>
        <span class="n">optimized_parameters</span><span class="p">,</span> <span class="o">*</span><span class="n">_</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">optimize</span><span class="o">.</span><span class="n">curve_fit</span><span class="p">(</span>
            <span class="n">f</span>       <span class="o">=</span> <span class="n">lorentzian</span><span class="p">,</span>
            <span class="n">xdata</span>   <span class="o">=</span> <span class="n">xfrq</span><span class="p">,</span> 
            <span class="n">ydata</span>   <span class="o">=</span> <span class="n">spectrum</span><span class="p">,</span> 
            <span class="n">method</span>  <span class="o">=</span> <span class="s1">&#39;lm&#39;</span><span class="p">,</span> 
            <span class="n">p0</span>      <span class="o">=</span> <span class="p">[</span><span class="n">frq_pred</span><span class="p">,</span> <span class="n">amp_pred</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>
        <span class="p">)</span>

        <span class="n">popt_table</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">optimized_parameters</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">popt_table</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>The table of best-fit parameters determined by <code class="docutils literal notranslate"><span class="pre">scipy.optimize.curve_fit()</span></code> is stored in the <code class="docutils literal notranslate"><span class="pre">opt_params</span></code> variable.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">opt_params</span> <span class="o">=</span> <span class="n">fit_spectrum_lorentzian</span><span class="p">(</span><span class="n">ft_table</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="id1">
<h2>The Neural Network<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h2>
<p>The ANN below is of similar architecture to the one used in Exercise 1. The major difference is the number of training parameters in the network. The input layer remains the same as the first network, but the final layer yields only 3 outputs - the location of the resonant frequency, the amplitude of the peak, and the width of the curve.</p>
<section id="exercise-2">
<h3>Exercise 2<a class="headerlink" href="#exercise-2" title="Permalink to this headline">#</a></h3>
<p>Unlike the linear transformation from an FID to its frequency spectrum in Exercise 1, there exist much more complex relationships between the input and output data in the following ANN due to the addition of a curve-fitting task which the network must learn.</p>
<p><br />
Activation functions are especially important for elucidating these complex relationships. Without them, the model behaves as if it only has a single layer and important weights cannot be updated as quickly. In short, the activation functions introduce non-linearity.</p>
<hr class="docutils" />
<p><strong>Exercise 2.1</strong> Revisit the activation functions introduced in Exercise 1. Tune the <code class="docutils literal notranslate"><span class="pre">activation</span></code> hyperparameter using some of the more advanced functions available in the <a class="reference external" href="https://keras.io/api/layers/activations/">Keras Activation Function API</a> documentation. Note how these functions alter the model’s ability to converge upon a solution.</p>
<p><br />
Aside from the activation function, all of the same hyperparameters from the network in Exercise 1.1 exist in this network, e.g. the number of layers and the number of nodes within each layer, the learning rate, momentum, etc.</p>
<hr class="docutils" />
<p><strong>Exercise 2.2</strong> This model is overfitting. Revisit the discussion on <strong>Regularization</strong> in Exercise 1, and tune the hyperparameters to seek some improvement. Change the <code class="docutils literal notranslate"><span class="pre">monitor</span></code> option in the early stopping callback to monitor <code class="docutils literal notranslate"><span class="pre">val_loss</span></code> instead of <code class="docutils literal notranslate"><span class="pre">loss</span></code>. In addition to the suggestions in Exercise 1, the model may be overly complex or more samples may be needed for training. Again, these are all tunable hyperparameters that may need adjustment over the course of designing the network. Check the tuning results against the figure at the end of this section - run it several times to compare against several randomly selected FIDs. If the results are unsatisfactory, tune the parameters again and continue to attempt to minimize <code class="docutils literal notranslate"><span class="pre">val_loss</span></code>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_lorentzian</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span>
        <span class="n">shape</span>       <span class="o">=</span> <span class="n">observed_fid_table</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="n">units</span>               <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">observed_fid_table</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="mi">2</span><span class="p">)),</span>
        <span class="n">activation</span>          <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
        <span class="n">use_bias</span>            <span class="o">=</span> <span class="kc">False</span>
    <span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="n">units</span>               <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">observed_fid_table</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="mi">4</span><span class="p">)),</span>
        <span class="n">activation</span>          <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu6</span><span class="p">,</span>
        <span class="n">use_bias</span>            <span class="o">=</span> <span class="kc">False</span>
    <span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="n">units</span>               <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">observed_fid_table</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="mi">8</span><span class="p">)),</span>
        <span class="n">activation</span>          <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu6</span><span class="p">,</span>
        <span class="n">use_bias</span>            <span class="o">=</span> <span class="kc">False</span>
    <span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="n">units</span>               <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">observed_fid_table</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="mi">16</span><span class="p">)),</span>
        <span class="n">activation</span>          <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">selu</span><span class="p">,</span>
        <span class="n">use_bias</span>            <span class="o">=</span> <span class="kc">False</span>
    <span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="n">units</span>               <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">observed_fid_table</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="mi">32</span><span class="p">)),</span>
        <span class="n">activation</span>          <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
        <span class="n">use_bias</span>            <span class="o">=</span> <span class="kc">False</span>
    <span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="n">units</span>               <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">observed_fid_table</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">/</span><span class="mi">128</span><span class="p">)),</span>
        <span class="n">activation</span>          <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span>
        <span class="n">use_bias</span>            <span class="o">=</span> <span class="kc">False</span>
    <span class="p">),</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span>
        <span class="n">activation</span>  <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="c1"># Linear activation in final later - regression-type problem</span>
        <span class="n">units</span>       <span class="o">=</span> <span class="n">opt_params</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
    <span class="p">)</span>
<span class="p">])</span>

<span class="n">lr_schedule_fit</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">schedules</span><span class="o">.</span><span class="n">ExponentialDecay</span><span class="p">(</span>
    <span class="n">initial_learning_rate</span>   <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">decay_steps</span>             <span class="o">=</span> <span class="mi">50</span><span class="p">,</span>
    <span class="n">decay_rate</span>              <span class="o">=</span> <span class="mf">0.96</span><span class="p">,</span>
    <span class="n">staircase</span>               <span class="o">=</span> <span class="kc">True</span>
    <span class="p">)</span>

<span class="n">early_stop_callback</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">EarlyStopping</span><span class="p">(</span>
    <span class="c1"># Change this to `val_loss` in order to complete Exercise 2.2</span>
    <span class="c1"># Will also need to uncomment `validation_split` in the `history = model_lorentzian.fit(...)` block.</span>
    <span class="n">monitor</span>   <span class="o">=</span> <span class="s1">&#39;loss&#39;</span><span class="p">,</span>
    <span class="n">min_delta</span> <span class="o">=</span> <span class="mf">1e-4</span><span class="p">,</span> 
    <span class="n">patience</span>  <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> 
    <span class="n">mode</span>      <span class="o">=</span> <span class="s1">&#39;auto&#39;</span><span class="p">,</span>
    <span class="n">verbose</span>   <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">baseline</span>  <span class="o">=</span> <span class="kc">None</span>
    <span class="p">)</span>

<span class="n">model_lorentzian</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">loss</span>        <span class="o">=</span> <span class="s1">&#39;mean_squared_error&#39;</span><span class="p">,</span> 
    <span class="n">optimizer</span>   <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span>
        <span class="n">learning_rate</span>   <span class="o">=</span> <span class="n">lr_schedule_fit</span><span class="p">,</span>
        <span class="n">momentum</span>        <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="n">clipnorm</span>        <span class="o">=</span> <span class="mf">1.0</span>
        <span class="p">)</span>
    <span class="p">)</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model_lorentzian</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">x</span>           <span class="o">=</span> <span class="n">observed_fid_table</span><span class="p">,</span> 
    <span class="n">y</span>           <span class="o">=</span> <span class="n">opt_params</span><span class="p">,</span> 
    <span class="n">epochs</span>      <span class="o">=</span> <span class="mi">300</span><span class="p">,</span> 
    <span class="n">batch_size</span>  <span class="o">=</span> <span class="mi">25</span><span class="p">,</span>
    <span class="c1"># Train on 80% of the input data, validate on 20%</span>
    <span class="c1"># validation_split  = 0.2,</span>
    <span class="n">callbacks</span>   <span class="o">=</span> <span class="p">[</span><span class="n">early_stop_callback</span><span class="p">]</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Consider the model summary below. Note how the number of training parameters is significantly smaller compared to the number of training parameters in the network in Exercise 1.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_lorentzian</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model: &quot;sequential_1&quot;
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 dense_2 (Dense)             (None, 2000)              8000000   
                                                                 
 dense_3 (Dense)             (None, 1000)              2000000   
                                                                 
 dense_4 (Dense)             (None, 500)               500000    
                                                                 
 dense_5 (Dense)             (None, 250)               125000    
                                                                 
 dense_6 (Dense)             (None, 125)               31250     
                                                                 
 dense_7 (Dense)             (None, 32)                4000      
                                                                 
 dense_8 (Dense)             (None, 3)                 99        
                                                                 
=================================================================
Total params: 10,660,349
Trainable params: 10,660,349
Non-trainable params: 0
_________________________________________________________________
</pre></div>
</div>
</div>
</div>
</section>
<section id="id2">
<h3>ANN Predictions<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h3>
<p>An FID may be submitted to the model via the <code class="docutils literal notranslate"><span class="pre">predict()</span></code> method, which will return the neural network’s estimation of the frequency spectrum’s best curve-fit parameters, in the form</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="n">frequency</span><span class="p">,</span> <span class="n">amplitude</span><span class="p">,</span> <span class="n">fwhm</span><span class="p">]</span>
</pre></div>
</div>
<p>where <code class="docutils literal notranslate"><span class="pre">amplitude</span></code> and <code class="docutils literal notranslate"><span class="pre">fwhm</span></code> correspond to the distribution-specific parameters (e.g. <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(\gamma\)</span> for the Lorentzian line shape) that influence those properties of the curve.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">nn_lorentzian</span><span class="p">(</span><span class="nb">input</span><span class="p">):</span>
  <span class="n">prediction</span> <span class="o">=</span> <span class="n">model_lorentzian</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="nb">input</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">newaxis</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
  <span class="k">return</span> <span class="n">prediction</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">#@title ANN Predictions Visualized</span>
<span class="c1">#@markdown Press the `Play` button on the left side of the cell to visualize the ANN predictions for a randomly generated FID. </span>
<span class="c1">#@markdown On the left is the submitted FID, in the center is a radar plot of the real and predicted fit parameters, and on the right is a superimposition of the ANN prediction and the real Lorentzian curve output for the best-fit parameters for the resonant frequency peak of that FID&#39;s FFT.</span>
<span class="c1">#@markdown Check the results of hyperparameter tuning against the output in this figure.</span>
<span class="c1">#@markdown</span>
<span class="c1">#@markdown ---</span>
<span class="c1">#@markdown **Exercise 1.1.6** Run this code several times and observe how the network&#39;s estimated best-fit parameters compare to those determined from the resonant frequency peak of the FID&#39;s FFT.</span>
<span class="c1"># Be patient - this function takes some time to run</span>
<span class="k">def</span> <span class="nf">all_nn_lorentzian</span><span class="p">(</span><span class="n">fid_input_table</span><span class="p">):</span>
  <span class="n">prediction_table</span> <span class="o">=</span> <span class="p">[]</span>

  <span class="k">for</span> <span class="n">fid</span> <span class="ow">in</span> <span class="n">fid_input_table</span><span class="p">:</span>
    <span class="n">fid_fit</span> <span class="o">=</span> <span class="n">nn_lorentzian</span><span class="p">(</span><span class="n">fid</span><span class="p">)</span>
    <span class="n">prediction_table</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">fid_fit</span><span class="p">)</span>

  <span class="k">return</span> <span class="n">prediction_table</span>

<span class="n">nn_test_fit_predictions</span> <span class="o">=</span> <span class="n">all_nn_lorentzian</span><span class="p">(</span><span class="n">test_fid_table</span><span class="p">)</span>
<span class="n">ft_test_fit_actual</span>  <span class="o">=</span> <span class="n">fit_spectrum_lorentzian</span><span class="p">(</span><span class="n">transform_fid_table</span><span class="p">(</span><span class="n">test_fid_table</span><span class="p">))</span>

<span class="n">total_sample_num</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">rf_mix_frequency</span><span class="p">)</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">spin2_relaxation</span><span class="p">)</span>
<span class="n">test_sample_num</span> <span class="o">=</span> <span class="n">total_sample_num</span> <span class="o">-</span> <span class="n">acq_fid_num</span>

<span class="n">random_sample_n</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">integers</span><span class="p">(</span><span class="n">low</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">high</span> <span class="o">=</span> <span class="n">test_sample_num</span><span class="p">)</span>

<span class="n">nn_curve_table</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">ft_curve_table</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">fit</span> <span class="ow">in</span> <span class="n">nn_test_fit_predictions</span><span class="p">:</span>
  <span class="n">nn</span> <span class="o">=</span> <span class="n">lorentzian</span><span class="p">(</span><span class="n">xfrq</span><span class="p">,</span> <span class="n">fit</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">fit</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">fit</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
  <span class="n">nn_curve_table</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">nn</span><span class="p">)</span>

<span class="k">for</span> <span class="n">fit</span> <span class="ow">in</span> <span class="n">ft_test_fit_actual</span><span class="p">:</span>
  <span class="n">ft</span> <span class="o">=</span> <span class="n">lorentzian</span><span class="p">(</span><span class="n">xfrq</span><span class="p">,</span> <span class="n">fit</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">fit</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">fit</span><span class="p">[</span><span class="mi">2</span><span class="p">])</span>
  <span class="n">ft_curve_table</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ft</span><span class="p">)</span>

<span class="n">random_fid</span>        <span class="o">=</span> <span class="n">test_fid_table</span><span class="p">[</span><span class="n">random_sample_n</span><span class="p">]</span>
<span class="n">random_nn_output</span>  <span class="o">=</span> <span class="n">nn_curve_table</span><span class="p">[</span><span class="n">random_sample_n</span><span class="p">]</span>
<span class="n">random_ft_output</span>  <span class="o">=</span> <span class="n">ft_curve_table</span><span class="p">[</span><span class="n">random_sample_n</span><span class="p">]</span>

<span class="c1">#*_, ax = plt.subplots(1, 2, figsize = (17, 5))</span>
<span class="n">fig</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">22</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">ax0</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">131</span><span class="p">)</span>
<span class="n">sb</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">time</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">random_fid</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax0</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="s1">&#39;FID&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">make_wrap</span><span class="p">(</span><span class="n">lst</span><span class="p">):</span>
  <span class="k">return</span> <span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="p">[</span><span class="o">*</span><span class="n">x</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]])(</span><span class="n">lst</span><span class="p">)</span>

<span class="n">curve_params</span> <span class="o">=</span> <span class="n">make_wrap</span><span class="p">([</span><span class="s1">&#39;Frequency&#39;</span><span class="p">,</span> <span class="s1">&#39;Amplitude&#39;</span><span class="p">,</span> <span class="s1">&#39;FWHM&#39;</span><span class="p">])</span>

<span class="n">nn_test_fit_predictions_mutate</span>  <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">nn_test_fit_predictions</span><span class="p">)</span>
<span class="n">ft_test_fit_actual_mutate</span>       <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">ft_test_fit_actual</span><span class="p">)</span>

<span class="k">for</span> <span class="n">prediction</span> <span class="ow">in</span> <span class="n">nn_test_fit_predictions_mutate</span><span class="p">:</span>
  <span class="n">prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">prediction</span><span class="p">[</span><span class="mi">0</span><span class="p">]))[</span><span class="o">-</span><span class="mi">2</span><span class="p">:])</span><span class="o">/</span><span class="mi">10</span>
  <span class="n">prediction</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*=</span> <span class="mi">2</span>
  <span class="c1">#prediction[0] /= 1000</span>
  <span class="c1">#prediction[0] = np.log(np.log(prediction[0]))</span>

<span class="k">for</span> <span class="n">bestfit</span> <span class="ow">in</span> <span class="n">ft_test_fit_actual_mutate</span><span class="p">:</span>
  <span class="n">bestfit</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">repr</span><span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">bestfit</span><span class="p">[</span><span class="mi">0</span><span class="p">]))[</span><span class="o">-</span><span class="mi">2</span><span class="p">:])</span><span class="o">/</span><span class="mi">10</span>
  <span class="n">bestfit</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*=</span> <span class="mi">2</span>
  <span class="c1">#bestfit[0] /= 1000</span>
  <span class="c1">#bestfit[0] = np.log(np.log(bestfit[0]))</span>

<span class="n">netw_params</span> <span class="o">=</span> <span class="n">make_wrap</span><span class="p">(</span><span class="n">nn_test_fit_predictions_mutate</span><span class="p">[</span><span class="n">random_sample_n</span><span class="p">])</span>
<span class="n">real_params</span> <span class="o">=</span> <span class="n">make_wrap</span><span class="p">(</span><span class="n">ft_test_fit_actual_mutate</span><span class="p">[</span><span class="n">random_sample_n</span><span class="p">])</span>

<span class="n">label_loc</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">stop</span><span class="o">=</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="n">num</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">netw_params</span><span class="p">))</span>

<span class="c1">#plt.figure(figsize=(8, 8))</span>
<span class="n">ax3</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">132</span><span class="p">,</span> <span class="n">polar</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label_loc</span><span class="p">,</span> <span class="n">netw_params</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;ANN Predicted Params&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">label_loc</span><span class="p">,</span> <span class="n">real_params</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;FFT Best-Fit Params&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Relative $\Delta$ of Real vs Predicted Lorentzian Fit Parameters&#39;</span><span class="p">)</span>
<span class="n">lines</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">thetagrids</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">degrees</span><span class="p">(</span><span class="n">label_loc</span><span class="p">),</span> <span class="n">labels</span><span class="o">=</span><span class="n">curve_params</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>


<span class="n">ax1</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">133</span><span class="p">)</span>
<span class="n">sb</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">xfrq</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">random_nn_output</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax1</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;ANN Prediction&#39;</span><span class="p">)</span>
<span class="n">sb</span><span class="o">.</span><span class="n">lineplot</span><span class="p">(</span><span class="n">x</span> <span class="o">=</span> <span class="n">xfrq</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">random_ft_output</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">ax1</span><span class="p">,</span> <span class="n">label</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;FFT Best-Fit Params&#39;</span><span class="p">)</span>


<span class="n">ax0</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;Observed FID&quot;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Time (s)&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Amplitude&quot;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="n">title</span><span class="o">=</span><span class="s2">&quot;Real-Parameter Lorentzian vs ANN Predicted Lorentzian&quot;</span><span class="p">,</span> <span class="n">xlabel</span><span class="o">=</span><span class="s2">&quot;Frequency (Hz)&quot;</span><span class="p">,</span> <span class="n">ylabel</span><span class="o">=</span><span class="s2">&quot;Amplitude&quot;</span><span class="p">)</span>

<span class="n">mn</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">([</span><span class="n">nn_test_fit_predictions</span><span class="p">[</span><span class="n">random_sample_n</span><span class="p">][</span><span class="mi">0</span><span class="p">],</span> <span class="n">ft_test_fit_actual</span><span class="p">[</span><span class="n">random_sample_n</span><span class="p">][</span><span class="mi">0</span><span class="p">]])</span>
<span class="n">xmin</span> <span class="o">=</span> <span class="n">mn</span> <span class="o">-</span> <span class="mi">15</span>
<span class="n">xmax</span> <span class="o">=</span> <span class="n">mn</span> <span class="o">+</span> <span class="mi">15</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">set_xlim</span><span class="p">(</span><span class="n">xmin</span><span class="p">,</span> <span class="n">xmax</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/45424d60a559ad2c9ee2946767c02945b18bfa33f8afe66433ed02fc1a9ec0bb.png" src="../../_images/45424d60a559ad2c9ee2946767c02945b18bfa33f8afe66433ed02fc1a9ec0bb.png" />
</div>
</div>
</section>
<section id="id3">
<h3>Discussion<a class="headerlink" href="#id3" title="Permalink to this headline">#</a></h3>
<p>Overall, the network trained on the curve fitting parameters reconstructs the frequency content of the FIDs with much higher fidelity - including the width and amplitude of the peak - than the network trained directly on the spectrum. Both networks display significant room for improvement; however, as was stressed earlier in this module, DFT computation is a poor application for a neural network due to the efficiency and exactness of modern FFT implementations. Nonetheless, the network in Exercise 2 that predicts parameters of best fit is a better problem domain for deep learning, and takes advantage of an ANN’s ability to learn complex, nonlinear relationships within data.</p>
<p><br />
Outside of the network hyperparameters, one possible area of improvement for the network is the line shape function to which the spectrum is fit. A Lorentzian distribution was utilized initially, but NMR spectra typically fit the Voigt profile better.</p>
<hr class="docutils" />
<p><strong>Exercise 2.3</strong> Implement the pseudo-Voigt profile as a <code class="docutils literal notranslate"><span class="pre">curve_fit()</span></code> callable function and fit the spectra. Specific details on SciPy curve fitting may be found in the <a class="reference external" href="https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.curve_fit.html">SciPy Optimize API</a> documentation. Train the network on the new fit parameters and investigate whether or not - and to what degree - the differences between the learned and real spectra have been resolved. Don’t forget to change the number of units in the output layer to account for any potential increase in parameters.</p>
<p><strong>Exercise 2.4</strong> Would the line shape function be considered a tunable hyperparameter?</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./DSECOP/NMR_Deep_Learning"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">NMR Spectrum Analysis Using Artificial Neural Networks</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#authored-by-sebastian-w-atalla">Authored by Sebastian W. Atalla</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#unc-chapel-hill-dept-of-physics-astronomy">UNC Chapel Hill | Dept. of Physics &amp; Astronomy</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#background"><strong>Background</strong></a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#deep-learning-applications">Deep Learning Applications</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#disclaimer"><strong>Disclaimer</strong></a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#preparation">Preparation</a></li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1-estimation-of-fid-frequency-content-from-fft"><strong>Exercise 1</strong> Estimation of FID Frequency Content from FFT</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#signal-data">Signal Data</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#the-neural-network">The Neural Network</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-1">Exercise 1</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#layers"><strong>Layers</strong></a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions"><strong>Activation Functions</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#linear-function"><strong>Linear Function</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#sigmoid-function"><strong>Sigmoid Function</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-function"><strong>Softmax Function</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#rectified-linear-unit-relu-function"><strong>Rectified Linear Unit (ReLU) Function</strong></a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization"><strong>Regularization</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#l1-and-l2-regularizers"><strong>L1 and L2 Regularizers</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#dropout"><strong>Dropout</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#early-stopping"><strong>Early Stopping</strong></a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizer"><strong>Optimizer</strong></a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#stochastic-gradient-descent"><strong>Stochastic Gradient Descent</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#learning-rate"><strong>Learning Rate</strong></a></li>
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#momentum"><strong>Momentum</strong></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ann-predictions">ANN Predictions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#discussion">Discussion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2-estimation-of-lorentzian-curve-fitting-parameters-from-fid-frequency-spectra"><strong>Exercise 2</strong> Estimation of Lorentzian Curve Fitting Parameters from FID Frequency Spectra</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#curve-fitting-of-nmr-data">Curve Fitting of NMR Data</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lorentzian-distribution"><strong>Lorentzian Distribution</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#gaussian-distribution"><strong>Gaussian Distribution</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#voigt-profile"><strong>Voigt Profile</strong></a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lorentzian-fit">Lorentzian Fit</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">The Neural Network</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#exercise-2">Exercise 2</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">ANN Predictions</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">Discussion</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>