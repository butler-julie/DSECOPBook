

<!DOCTYPE html>


<html >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Notebook 2: What is a Neural Network? &#8212; My sample book</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'DSECOP/Solving_Differential_Equations_with_NNs/02_neural_networks';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="None"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="../../intro.html">
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../intro.html">
                    Table of Contents
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Exploratory Data Analysis</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Exploratory_Data_Analysis/README.html">Exploratory Data Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Exploratory_Data_Analysis/01_dataset_cleaning.html">Exploratory Data Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Exploratory_Data_Analysis/02_preprocessing_techniques.html">Preprocessing Techniques for Machine Learning</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FDSECOP/Solving_Differential_Equations_with_NNs/02_neural_networks.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/DSECOP/Solving_Differential_Equations_with_NNs/02_neural_networks.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Notebook 2: What is a Neural Network?</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-neural-network">What is a neural network?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-terminology">Neural Network Terminology</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fully-connected-feedforward-neural-network-fully-connected-ffnn">Fully Connected Feedforward Neural Network (Fully Connected FFNN)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematics-of-a-neuron">Mathematics of  a Neuron</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions">Activation Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-note-on-hyperparameters">A Note on Hyperparameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematics-of-neural-networks-the-first-hidden-layer">Mathematics of Neural Networks: The First Hidden Layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematics-of-neural-networks-the-second-hidden-layer">Mathematics of Neural Networks: The Second Hidden Layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematics-of-neural-networks-the-l-th-hidden-layer">Mathematics of Neural Networks: The l-th Hidden Layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-loss-function">Neural Network Loss Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-optimized-weights-and-biases">Finding The Optimized Weights and Biases</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass">Forward Pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation">Backpropagation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-neural-network-from-scratch-using-jax">Creating a Neural Network from Scratch Using JAX</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-the-data-set">Generate the Data Set</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#perform-a-train-test-split">Perform a Train-Test Split</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-neural-network">Define the Neural Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-loss-function">Define the Loss Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-the-neural-network">Train the Neural Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analyze-the-results">Analyze the Results</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks-with-a-popular-python-library">Neural Networks with a Popular Python Library</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#keras">Keras</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practice-what-you-have-learned">Practice What You Have Learned</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <p><a href="https://colab.research.google.com/github/GDS-Education-Community-of-Practice/DSECOP/blob/main/Solving_Differential_Equations_with_NNs/02_neural_networks.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a></p>
<section class="tex2jax_ignore mathjax_ignore" id="notebook-2-what-is-a-neural-network">
<h1>Notebook 2: What is a Neural Network?<a class="headerlink" href="#notebook-2-what-is-a-neural-network" title="Permalink to this headline">#</a></h1>
<p>This notebook will briefly go through the aspects of neural networks that will important for this application.  For a more general overview of neural networks, please see the set of lectures and exercises located <a class="reference external" href="https://github.com/GDS-Education-Community-of-Practice/DSECOP/tree/IntroDeepLearn/IntroDeepLearning/lectures">here</a>.</p>
<section id="what-is-a-neural-network">
<h2>What is a neural network?<a class="headerlink" href="#what-is-a-neural-network" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p>Neural networks can be defined as computational systems that can learn to perform tasks by considering examples, generally without being programmed with any task-specific rules. Another way to phrase this is that a neural network is a computational system that learns to match a given input to the correct output. They are a broad category of machine learning algorithms that include popular algorithms such as convolutional neural networks, recurrent neural networks, and deep learning.</p></li>
</ul>
</section>
<section id="neural-network-terminology">
<h2>Neural Network Terminology<a class="headerlink" href="#neural-network-terminology" title="Permalink to this headline">#</a></h2>
<p><img alt="NN" src="https://raw.githubusercontent.com/GDS-Education-Community-of-Practice/DSECOP/main/Solving_Differential_Equations_with_NNs/nn.png" /></p>
<ul class="simple">
<li><p>Neuron (Node): the simplest unit of a neural network, takes in an input and produces an output.  Neurons are represent by circles in the above diagram.</p></li>
<li><p>Layer: a collection of neurons that act together.  Layers are represented by vertical stacks of neurons in the above diagram.</p></li>
<li><p>Input Layer: the first layer in a neural network, performs no manipulations to the data.  The layer to the very left in the above diagram is the input layer.</p></li>
<li><p>Output Layer: the last layer of a neural network.  The layer to the very left in the above diagram is the output layer.</p></li>
<li><p>Hidden Layer: any other layer of a neural network, in between the input and output layers.  All other layers besides the input and output layers in the above diagram are hidden layers.</p></li>
</ul>
</section>
<section id="fully-connected-feedforward-neural-network-fully-connected-ffnn">
<h2>Fully Connected Feedforward Neural Network (Fully Connected FFNN)<a class="headerlink" href="#fully-connected-feedforward-neural-network-fully-connected-ffnn" title="Permalink to this headline">#</a></h2>
<p><img alt="NN" src="https://raw.githubusercontent.com/GDS-Education-Community-of-Practice/DSECOP/main/Solving_Differential_Equations_with_NNs/nn.png" /></p>
<p>Though there are many types of neural network, using just the phrase “neural network” typically refers to a type of neural network known as a fully connected feedforward neural network (FFNN).  This type of network can also be known as a multilayer perceptron (MLP) if it has at least one hidden layer.</p>
<p>Information in an FFNN moves only forward (left to right in the above diagram).  Additionally each neuron is connected to every neuron in the next layer and there are no connection between neurons in the same layer.  This means that the input to a layer in the neural network is simply the output from the previous layer.  As we will see in a moment, each neuron receives a weighted sum of the outputs of all neurons in the previous layer.</p>
</section>
<section id="mathematics-of-a-neuron">
<h2>Mathematics of  a Neuron<a class="headerlink" href="#mathematics-of-a-neuron" title="Permalink to this headline">#</a></h2>
<p>Each neuron is a mathematical function involving a column from a weight matrix, a scalar from a bias vector, and an activation function.  We can represent the mathematical form of the i-th neuron as:</p>
<div class="math notranslate nohighlight">
\[\hat{y}_i = f(\sum_{j=1}^M w_{ij}x_j + b_i),\]</div>
<p>where x is the input to the neural network, w is the weight matrix which scales the input of the neuron, b is the bias vector that makes sure the output of the neuron is non-zero, and f is known as the activation function, which adds nonlinearity to the neuron.</p>
</section>
<section id="activation-functions">
<h2>Activation Functions<a class="headerlink" href="#activation-functions" title="Permalink to this headline">#</a></h2>
<p>There are many common function to use as activation functions for neural networks, but three of the most commons ones are listed here.</p>
<p><strong>Sigmoid</strong></p>
<div class="math notranslate nohighlight">
\[f(x) = \frac{1}{1 + e^{-x}}\]</div>
<p><strong>Hyperbolic Tangent</strong></p>
<div class="math notranslate nohighlight">
\[f(x) = tanh(x) = \frac{e^{2x} - 1}{e^{2x} + 1}\]</div>
<p><strong>Relu (Rectified Linear Unit)</strong></p>
<div class="math notranslate nohighlight">
\[f(x) = max(0, x)\]</div>
</section>
<section id="a-note-on-hyperparameters">
<h2>A Note on Hyperparameters<a class="headerlink" href="#a-note-on-hyperparameters" title="Permalink to this headline">#</a></h2>
<p>The number of hidden layers, number of neurons per layer, activation function, and many other features are called hyperparameters of a neural network, meaning that their values must be chosen by the user before the network is run. Changing the value of a hyperparameter can drastically change the results of the neural network.</p>
</section>
<section id="mathematics-of-neural-networks-the-first-hidden-layer">
<h2>Mathematics of Neural Networks: The First Hidden Layer<a class="headerlink" href="#mathematics-of-neural-networks-the-first-hidden-layer" title="Permalink to this headline">#</a></h2>
<p>For each neuron, i, in the first hidden layer of a neural network, we can represent its mathematical form as:</p>
<div class="math notranslate nohighlight">
\[\hat{y}_i^1 = f^1(\sum_{j=1}^M w_{ij}^1x_j + b_i^1).\]</div>
<p>We can also write out the mathematical form for the entire first hidden layer as:</p>
<div class="math notranslate nohighlight">
\[\hat{y}_1 = f^1(W_1\textbf{x} + \textbf{b}_1)\]</div>
</section>
<section id="mathematics-of-neural-networks-the-second-hidden-layer">
<h2>Mathematics of Neural Networks: The Second Hidden Layer<a class="headerlink" href="#mathematics-of-neural-networks-the-second-hidden-layer" title="Permalink to this headline">#</a></h2>
<p>Similarly for the second hidden layer, we can represent the mathematical form of each neuron as:</p>
<div class="math notranslate nohighlight">
\[y_i^2 = f^2(\sum_{j=1}^N w_{ij}^2y_j^1 + b_i^2).\]</div>
<p>Note here that the weights matriz is no longer multiplied by the inputs to the neural network (x), but rather to the output of the first hidden layer.  This is because the input to the first hidden layer is the input ot the neural network but the input to the second hidden layer is the output of the first hidden layer.  Therefore we can expand the above equation to be a bit more clear:</p>
<div class="math notranslate nohighlight">
\[y_i^2 = f^2(\sum_{j=1}^N w_{ij}^2f^1(\sum_{k=1}^M w_{kj}^1x_k + b_j^1) + b_i^2).\]</div>
<p>We can also write a mathematical form for the entire second hidden layer as:</p>
<div class="math notranslate nohighlight">
\[\hat{y}_2 = f^2(W_2\hat{y}_1 + \textbf{b}_2),\]</div>
<p>or more explicitly as</p>
<div class="math notranslate nohighlight">
\[\hat{y}_2 = f^2(W_2f^1(W_1\textbf(x) + \textbf{b}_1) + \textbf{b}_2).\]</div>
</section>
<section id="mathematics-of-neural-networks-the-l-th-hidden-layer">
<h2>Mathematics of Neural Networks: The l-th Hidden Layer<a class="headerlink" href="#mathematics-of-neural-networks-the-l-th-hidden-layer" title="Permalink to this headline">#</a></h2>
<p>Finally, we can use the pattern we have developed to write down the equation for the mathematical output for a neuron on the l-th hidden layer of the neural network.  For the i-th neuron on the l-th layer we can describe it mathematically as:</p>
<div class="math notranslate nohighlight">
\[y_i^l = f^l(\sum_{j=1}^{N_l} w_{ij}^ly_j^{l-1} + b_i^l),\]</div>
<p>and more explicitly as</p>
<div class="math notranslate nohighlight">
\[y_i^l = f^l(\sum_{j=1}^{N_l} w_{ij}^lf^{l-1}(\sum_{k=1}^{N_{l-1}} w_{kj}^{l-2}y_k^{l-1} + b_j^{l-1}) + b_i^l),\]</div>
<p>and finally all the way expanded as</p>
<div class="math notranslate nohighlight">
\[y_i^l = f^l(\sum_{j=1}^{N_l} w_{ij}^l f^{l-1}(\sum_{k=1}^{N_{l-1}} w_{jk}^{l-1}( \cdot \cdot \cdot f^1(\sum_{n=1}^M w_{mn}^1x_n + b_m^1) \cdot \cdot \cdot ) + b_k^{l-1}) + b_j^l).\]</div>
<p>We can also write a mathematical expression for the output of the entire l-th layer as:</p>
<div class="math notranslate nohighlight">
\[\hat{y}_l = f^l (W_l \hat{y}_{l-1} + \textbf{b}_l),\]</div>
<p>which can be expanded to</p>
<div class="math notranslate nohighlight">
\[\hat{y}_l = f^l(W_lf^{l-1}(W_{l-1}\hat{y}_{l-2} + \textbf{b}_{l-1}) + \textbf{b}_l),\]</div>
<p>and finally to</p>
<div class="math notranslate nohighlight">
\[\hat{y}_l = f^l(W_lf^{l-1}(W_{l-1}(\cdot \cdot \cdot f^1(W_1\textbf{x} + \textbf{b}_1) \cdot \cdot \cdot) + \textbf{b}_{l-1}) + \textbf{b}_l).\]</div>
<p>It’s a complicated expression and can grow to be very large but it is a set equation that describes the output of a neural network with l-1 hidden layers and an output layer. So it is also possible to rephrase the definition of a neural network to be an analytical function that maps a set of inputs to a set of outputs using a set of optimized parameters.</p>
</section>
<section id="neural-network-loss-function">
<h2>Neural Network Loss Function<a class="headerlink" href="#neural-network-loss-function" title="Permalink to this headline">#</a></h2>
<p>A loss function is used to determine how much the output from a neural network differs from the true/expected result.  There is not a set loss function that is used with neural networks, but two common loss functions are the mean-squared error loss function and the mean absolute error function.  The mean-squared error loss function (MSE) can be defined as:</p>
<div class="math notranslate nohighlight">
\[J_{MSE}(W) = \frac{1}{N}\sum_{i=1}^N (y_i - \hat{y}_i)^2,\]</div>
<p>where y is the true data set, <span class="math notranslate nohighlight">\(\hat{y}\)</span> is the neural network prediction, N is the number of data points in the set, and W are the weights of the neural network.  The loss function depends on the weights of the neural network because changing the weights of the neural network changes its output.</p>
<p>The mean-absolute error loss function (MAE) has a similar form:</p>
<div class="math notranslate nohighlight">
\[J_{MAE}(W) = \frac{1}{N}\sum_{i=1}^N |y_i - \hat{y}_i|..\]</div>
</section>
<section id="finding-the-optimized-weights-and-biases">
<h2>Finding The Optimized Weights and Biases<a class="headerlink" href="#finding-the-optimized-weights-and-biases" title="Permalink to this headline">#</a></h2>
<p>A major part of working with neural networks is a process known as training where the weights of the neural network are optimized such that the cost function is minimized.  This training process has two phases: the forward pass and the backpropagation.</p>
</section>
<section id="forward-pass">
<h2>Forward Pass<a class="headerlink" href="#forward-pass" title="Permalink to this headline">#</a></h2>
<p><img alt="NN" src="https://raw.githubusercontent.com/GDS-Education-Community-of-Practice/DSECOP/main/Solving_Differential_Equations_with_NNs/bp1.png" /></p>
<p>The forward pass occurs when data is sent through the neural network (from left to right on the above graph) to produce a predicted output.  This predicted output is then fed into the loss function with the true data set to generate the loss value.</p>
</section>
<section id="backpropagation">
<h2>Backpropagation<a class="headerlink" href="#backpropagation" title="Permalink to this headline">#</a></h2>
<p><img alt="NN" src="https://raw.githubusercontent.com/GDS-Education-Community-of-Practice/DSECOP/main/Solving_Differential_Equations_with_NNs/bp2.png" /></p>
<p>After the forward pass comes backpropagation, where the error from the loss function is backpropagated through the layers of the neural networks and its weights are adjusted layer by layer so that the next forward pass will result in a reduced loss value.  A simple way to optimize the weights of a neural network during backpropagation is through an optimization technique known as gradient descent.  The weights of the neural network are simply adjusted by the derivative of the loss function with respect to the weights, scaled by a hyperparameter known as the learning rate:</p>
<div class="math notranslate nohighlight">
\[W = W - r_{l}\frac{\partial J(W)}{\partial W}.\]</div>
<p>The learning rate (r<span class="math notranslate nohighlight">\(_l\)</span>) is a number typically much less than 1 and it is also a hyperparameter, so its value must be set before the neural network is run.</p>
<p>The process of training a neural network involves many different iterations of forward pass followed by backpropagation.  Typically a training process will continue until a certain number of training iterations has been reached or the difference in the current loss value compared to the value from the previous iteration is below a certain threshold.  However, neural networks should not be trained for an overly long time because this will lead to something called overfitting where the neural network learns to match the data set it is trained with very well (so will show a small loss value) but loses all generality when given new data (so it will perform poorly when given the new data set).</p>
<p><strong>EXERCISE 1</strong>: Take a moment and summarize what you have learned about neural networks in the text box below.</p>
<p>Delete this text and type your response here.</p>
</section>
<section id="creating-a-neural-network-from-scratch-using-jax">
<h2>Creating a Neural Network from Scratch Using JAX<a class="headerlink" href="#creating-a-neural-network-from-scratch-using-jax" title="Permalink to this headline">#</a></h2>
<p>JAX is an automatic differentiation library in Python that can find the derivative of any chunk of code it is given.  If you are interested you can read more about the library <a class="reference external" href="https://github.com/google/jax">here</a>.</p>
<p>The below section of the notebook will create a neural network entirely from scratch and analyze its performance. We will be using the gradient feature of the JAX library to implement a gradient descent optimization of our loss function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># IMPORTS</span>
<span class="c1"># Math for the ceiling function</span>
<span class="kn">from</span> <span class="nn">math</span> <span class="kn">import</span> <span class="n">ceil</span>
<span class="c1"># Matplotlib for graphing capabilities</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="c1"># Numpy for arrays</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="c1"># Modules from the JAX library for creating neural networks</span>
<span class="kn">import</span> <span class="nn">jax.numpy</span> <span class="k">as</span> <span class="nn">jnp</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">grad</span>
<span class="kn">from</span> <span class="nn">jax</span> <span class="kn">import</span> <span class="n">random</span> <span class="k">as</span> <span class="n">npr</span>
</pre></div>
</div>
</div>
</div>
<section id="generate-the-data-set">
<h3>Generate the Data Set<a class="headerlink" href="#generate-the-data-set" title="Permalink to this headline">#</a></h3>
<p>Let’s keep things simple and generate a data points from a Gaussian curve.  We will have our x data be evenly space between -10 and 10 and our y data be the corresponding points on a Gaussian curve.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Let&#39;s create a data set that is just a basic Gaussian curve</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="o">-</span><span class="mi">10</span><span class="p">,</span><span class="mi">10</span><span class="p">,</span><span class="mi">250</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">X</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="perform-a-train-test-split">
<h3>Perform a Train-Test Split<a class="headerlink" href="#perform-a-train-test-split" title="Permalink to this headline">#</a></h3>
<p>In machine learning problems it is common to split your data into two sets.  The first set, which usually contains 70%-80% of the data, is called the training set.  This set of data is used to train the machine learning algorithm (i.e. set the weights such that the cost function is minimized).  The second data set, which is much smaller, is called the test set.  This is used to test the accuracy of the machine learning algorithm on data that it has not yet seen.</p>
<p>Below we use the common training-test split functionality from the library Scikit-Learn.  We will be using 80% of our total data set as the training set with the remaining 20% being the test set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># We will split the data set into two pieces, a training data set that contains</span>
<span class="c1"># 80% of the total data and a test set that contains the other 20%</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>
<span class="n">train_size</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">train_size</span><span class="o">=</span><span class="n">train_size</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="define-the-neural-network">
<h3>Define the Neural Network<a class="headerlink" href="#define-the-neural-network" title="Permalink to this headline">#</a></h3>
<p>Now we will define a neural network using the equations for neural networks defined above.  First we will define the sigmoid function as our activation function.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Calculates the value of the sigmoid function for </span>
<span class="sd">        a given input of x</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="mf">1.</span> <span class="o">/</span> <span class="p">(</span><span class="mf">1.</span> <span class="o">+</span> <span class="n">jnp</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<p>Now we will define our neural network.  Here we will be using an architecture with two hidden layers, each using the sigmoid activation function, and an output layer which does not have an activation function.  Note how the input to the second hidden layer is the output from the first hidden layer and the input to the output layer is the output from the second hidden layer.  We will be setting the number of neurons per layer later when we are training the neural network.  The code we have defined below does not require a specific number of neurons per hidden layer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">neural_network</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Inputs:</span>
<span class="sd">            W (a list of length 2): the weights of the neural network</span>
<span class="sd">            x (a float): the input value of the neural network</span>
<span class="sd">        Returns:</span>
<span class="sd">            Unnamed (a float): The output of the neural network</span>
<span class="sd">        Defines a neural network with one hidden layer.  The number of neurons in</span>
<span class="sd">        the hidden layer is the length of W[0]. The activation function is the </span>
<span class="sd">        sigmoid function on the hidden layer an none on the output layer.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Calculate the output for the neurons in the hidden layers</span>
    <span class="n">hidden_layer1</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">W</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">hidden_layer2</span> <span class="o">=</span> <span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">hidden_layer1</span><span class="p">,</span> <span class="n">W</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
    <span class="c1"># Calculate the result for the output neuron</span>
    <span class="k">return</span> <span class="n">jnp</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">jnp</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">hidden_layer2</span><span class="p">,</span> <span class="n">W</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="define-the-loss-function">
<h3>Define the Loss Function<a class="headerlink" href="#define-the-loss-function" title="Permalink to this headline">#</a></h3>
<p>Now we need to define our loss function.  For simplicity we will be using the mean-squared error loss function, which is a very common loss function for training neural networks.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">loss_function</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Inputs:</span>
<span class="sd">            W (a list): the weights of the neural network</span>
<span class="sd">            t (a 1D NumPy array): the times to calculate the predicted position at</span>
<span class="sd">        Returns:</span>
<span class="sd">            loss_sum (a float): The total loss over all times</span>
<span class="sd">        The loss function for the neural network to solve for position given </span>
<span class="sd">        a function for acceleration.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Define a variable to hold the total loss</span>
    <span class="n">loss_sum</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="c1"># Loop through each individual time</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)):</span>
        <span class="c1"># Get the output of the neural network with the given set of weights</span>
        <span class="n">nn</span> <span class="o">=</span> <span class="n">neural_network</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">[</span><span class="n">i</span><span class="p">])[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">err_sqr</span> <span class="o">=</span> <span class="p">(</span><span class="n">nn</span><span class="o">-</span><span class="n">y</span><span class="p">[</span><span class="n">i</span><span class="p">])</span><span class="o">**</span><span class="mi">2</span>
        <span class="c1"># Update the loss sum</span>
        <span class="n">loss_sum</span> <span class="o">+=</span> <span class="n">err_sqr</span>
    <span class="n">loss_sum</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="c1"># Return the loss sum    </span>
    <span class="k">return</span> <span class="n">loss_sum</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="train-the-neural-network">
<h3>Train the Neural Network<a class="headerlink" href="#train-the-neural-network" title="Permalink to this headline">#</a></h3>
<p>Finally we need to train our neural network.  We will start by randomly initializing the weights of our neural network (with 25 neurons per hidden layer).  We then define the parameters for the learning rate, the number of training iterations, and the threshold for stopping the training.  Next, we perform gradient descent to update the weights of the neural network over for the set number of training iterations, or until the loss function value converges to some set threshold.</p>
<p><strong>WARNING</strong>: This cell will take a long time to run.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Generate the key for the random number generator</span>
<span class="n">key</span> <span class="o">=</span> <span class="n">npr</span><span class="o">.</span><span class="n">PRNGKey</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="c1"># Set the number of neurons in the hidden layer</span>
<span class="n">number_hidden_neurons</span> <span class="o">=</span> <span class="mi">25</span>
<span class="c1"># Initialize the weights of the neural network with random numbers</span>
<span class="n">W</span> <span class="o">=</span> <span class="p">[</span><span class="n">npr</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,(</span><span class="mi">1</span><span class="p">,</span> <span class="n">number_hidden_neurons</span><span class="p">)),</span> 
     <span class="n">npr</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,(</span><span class="n">number_hidden_neurons</span><span class="p">,</span><span class="n">number_hidden_neurons</span><span class="p">)),</span> 
     <span class="n">npr</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">key</span><span class="p">,(</span><span class="n">number_hidden_neurons</span><span class="p">,</span> <span class="mi">1</span><span class="p">))]</span>

<span class="c1"># Set the learning rate and the number of training iterations for the network</span>
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">num_training_iterations</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">threshold</span> <span class="o">=</span> <span class="mf">0.0001</span>
<span class="n">previous_loss</span> <span class="o">=</span> <span class="mi">0</span>

<span class="c1"># Train the neural network for the specified number of iterations</span>
<span class="c1"># Update the weights using the learning rates</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_training_iterations</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Training Iteration:&quot;</span><span class="p">,</span> <span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">current_loss</span> <span class="o">=</span> <span class="n">loss_function</span><span class="p">(</span><span class="n">W</span><span class="p">,</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Loss:&quot;</span><span class="p">,</span> <span class="n">current_loss</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">()</span>
    <span class="c1"># If the current loss is within a set threshold of the previous loss, stop</span>
    <span class="c1"># the training</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">current_loss</span><span class="o">-</span><span class="n">previous_loss</span><span class="p">)</span> <span class="o">&lt;</span> <span class="n">threshold</span><span class="p">:</span>
        <span class="k">break</span><span class="p">;</span>
    <span class="c1"># Calculate the gradient of the loss function and then use that gradient to</span>
    <span class="c1"># update the weights of the neural network using the learning rate and the </span>
    <span class="c1"># gradient descent optimization method</span>
    <span class="n">loss_grad</span> <span class="o">=</span>  <span class="n">grad</span><span class="p">(</span><span class="n">loss_function</span><span class="p">)(</span><span class="n">W</span><span class="p">,</span> <span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
    <span class="n">W</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">loss_grad</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">W</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">loss_grad</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">W</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span> <span class="o">-</span> <span class="n">learning_rate</span> <span class="o">*</span> <span class="n">loss_grad</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">previous_loss</span> <span class="o">=</span> <span class="n">current_loss</span>
    
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training Iteration: 1
Loss: 0.9966625
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training Iteration: 2
Loss: 0.9049263
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training Iteration: 3
Loss: 0.46874812
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training Iteration: 4
Loss: 0.57576776
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training Iteration: 5
Loss: 0.32366684
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training Iteration: 6
Loss: 0.39632678
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Training Iteration: 7
Loss: 0.39625087
</pre></div>
</div>
</div>
</div>
</section>
<section id="analyze-the-results">
<h3>Analyze the Results<a class="headerlink" href="#analyze-the-results" title="Permalink to this headline">#</a></h3>
<p>Now we need to analyze the performance of our neural network using the test data set that was reserved earlier.  First we need to generate the neural network predictions for the y component of the test data set.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">y_nn</span> <span class="o">=</span> <span class="p">[</span><span class="n">neural_network</span><span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">xi</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">xi</span> <span class="ow">in</span> <span class="n">X_test</span><span class="p">]</span> 
</pre></div>
</div>
</div>
</div>
<p>First let’s analyze the results graphically by plotting the predicted test data set and the true test data set on the same graph.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_nn</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;NN Prediction&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">,</span> <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;red&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;True Data&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">12</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s2">&quot;X&quot;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s2">&quot;y&quot;</span><span class="p">,</span><span class="n">fontsize</span><span class="o">=</span><span class="mi">14</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Text(0, 0.5, &#39;y&#39;)
</pre></div>
</div>
<img alt="../../_images/153affea40c7acf82b6f046cc27c228e4c0bafe8dba7d0513fa62a9b8a75cf65.png" src="../../_images/153affea40c7acf82b6f046cc27c228e4c0bafe8dba7d0513fa62a9b8a75cf65.png" />
</div>
</div>
<p>Next let’s analyze the error numerically using the root mean-squared error (RMSE) function, which is simply the square root of the mean-squared error.  The RMSE gives the average error on each data point (instead of the squared average error) so it is a met more of a clear metric for error analysis.  First, let’s define a function to calculate the RMSE between two data sets.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">rmse</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="n">B</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Inputs:</span>
<span class="sd">            A,B (NumPy arrays)</span>
<span class="sd">        Returns:</span>
<span class="sd">            Unnamed (a float): the RMSE error between A and B</span>
<span class="sd">        Calculates the RMSE error between A and B.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="o">==</span><span class="nb">len</span><span class="p">(</span><span class="n">B</span><span class="p">),</span><span class="s2">&quot;The data sets must be the same length to calcualte</span><span class="se">\</span>
<span class="s2">        the RMSE.&quot;</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">average</span><span class="p">((</span><span class="n">A</span><span class="o">-</span><span class="n">B</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span><span class="p">))</span> 
</pre></div>
</div>
</div>
</div>
<p>Now let’s print the RMSE between the true test data set and the neural network prediction.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="nb">print</span><span class="p">(</span><span class="s2">&quot;RMSE between true test set and neural network result:&quot;</span><span class="p">,</span> <span class="n">rmse</span><span class="p">(</span><span class="n">y_nn</span><span class="p">,</span><span class="n">y_test</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>RMSE between true test set and neural network result: 0.6314543413296326
</pre></div>
</div>
</div>
</div>
<p><strong>EXERCISE 2</strong>: Are you satisfied with the performance of this neural network?  If not, brainstorm some ways in which its performance may be improved.</p>
<p>Delete this text and type your response here.</p>
</section>
</section>
<section id="neural-networks-with-a-popular-python-library">
<h2>Neural Networks with a Popular Python Library<a class="headerlink" href="#neural-networks-with-a-popular-python-library" title="Permalink to this headline">#</a></h2>
<p>When using a neural network for most cases, instead of creating one by hand you will likely use a neural network implementation from a popular Python library.  These have several advantages including being able to easily use large networks (which would be hard to create by hand), the use of more advanced optimizers for training, and more optimized implementations to significantly decrease runtime (and likely more accuracy).</p>
<section id="keras">
<h3>Keras<a class="headerlink" href="#keras" title="Permalink to this headline">#</a></h3>
<p>One of the most popular machine learning library in Python is known as Keras, which is actually a wrapper for another machine learning library known as Tensorflow.  See the <a class="reference external" href="https://keras.io/">Keras website</a> for more information.   Keras is very commonly used for training neural networks because it makes building and training neural networks simply and intuitive.</p>
<p>Neural networks are build in keras by adding layers with the specified parameters to a model and then compiling the model with a loss function and an optimizer.  Here we use the same architecture as we have been using (two hidden layers with 25 neurons each and a hyperbolic tangent activation function).  The main difference here is that instead of using the gradient descent algorithm the optimize the weights of the neural network we will instead be using a much more powerful optimizer known as the Adam optimizer.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">keras</span>

<span class="c1"># Create the model </span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span> <span class="o">=</span> <span class="mi">25</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;tanh&#39;</span><span class="p">,</span> <span class="n">input_dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span> <span class="o">=</span> <span class="mi">25</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;tanh&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">units</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;tanh&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;mse&#39;</span><span class="p">,</span> <span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;adam&quot;</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span><span class="n">y_train</span><span class="p">,</span><span class="n">epochs</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
<span class="n">y_nn</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">rmse</span><span class="p">(</span><span class="n">y_nn</span><span class="p">,</span> <span class="n">y_test</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_nn</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span><span class="n">y_test</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output traceback highlight-ipythontb notranslate"><div class="highlight"><pre><span></span><span class="gt">---------------------------------------------------------------------------</span>
<span class="ne">ModuleNotFoundError</span><span class="g g-Whitespace">                       </span>Traceback (most recent call last)
<span class="n">Cell</span> <span class="n">In</span> <span class="p">[</span><span class="mi">12</span><span class="p">],</span> <span class="n">line</span> <span class="mi">1</span>
<span class="ne">----&gt; </span><span class="mi">1</span> <span class="kn">import</span> <span class="nn">keras</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="c1"># Create the model </span>
<span class="g g-Whitespace">      </span><span class="mi">4</span> <span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">()</span>

<span class="n">File</span> <span class="o">~/</span><span class="n">Library</span><span class="o">/</span><span class="n">Python</span><span class="o">/</span><span class="mf">3.9</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">keras</span><span class="o">/</span><span class="fm">__init__</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">20</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="c1"># Copyright 2015 The TensorFlow Authors. All Rights Reserved.</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="c1">#</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">13</span> <span class="c1"># limitations under the License.</span>
<span class="g g-Whitespace">     </span><span class="mi">14</span> <span class="c1"># ==============================================================================</span>
<span class="g g-Whitespace">     </span><span class="mi">15</span> <span class="sd">&quot;&quot;&quot;Implementation of the Keras API, the high-level API of TensorFlow.</span>
<span class="g g-Whitespace">     </span><span class="mi">16</span><span class="sd"> </span>
<span class="g g-Whitespace">     </span><span class="mi">17</span><span class="sd"> Detailed documentation and user guides are available at</span>
<span class="g g-Whitespace">     </span><span class="mi">18</span><span class="sd"> [keras.io](https://keras.io).</span>
<span class="g g-Whitespace">     </span><span class="mi">19</span><span class="sd"> &quot;&quot;&quot;</span>
<span class="ne">---&gt; </span><span class="mi">20</span> <span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">distribute</span>
<span class="g g-Whitespace">     </span><span class="mi">21</span> <span class="kn">from</span> <span class="nn">keras</span> <span class="kn">import</span> <span class="n">models</span>
<span class="g g-Whitespace">     </span><span class="mi">22</span> <span class="kn">from</span> <span class="nn">keras.engine.input_layer</span> <span class="kn">import</span> <span class="n">Input</span>

<span class="n">File</span> <span class="o">~/</span><span class="n">Library</span><span class="o">/</span><span class="n">Python</span><span class="o">/</span><span class="mf">3.9</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">keras</span><span class="o">/</span><span class="n">distribute</span><span class="o">/</span><span class="fm">__init__</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">18</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="c1"># Copyright 2019 The TensorFlow Authors. All Rights Reserved.</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="c1">#</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">13</span> <span class="c1"># limitations under the License.</span>
<span class="g g-Whitespace">     </span><span class="mi">14</span> <span class="c1"># ==============================================================================</span>
<span class="g g-Whitespace">     </span><span class="mi">15</span> <span class="sd">&quot;&quot;&quot;Keras&#39; Distribution Strategy library.&quot;&quot;&quot;</span>
<span class="ne">---&gt; </span><span class="mi">18</span> <span class="kn">from</span> <span class="nn">keras.distribute</span> <span class="kn">import</span> <span class="n">sidecar_evaluator</span>

<span class="n">File</span> <span class="o">~/</span><span class="n">Library</span><span class="o">/</span><span class="n">Python</span><span class="o">/</span><span class="mf">3.9</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">python</span><span class="o">/</span><span class="n">site</span><span class="o">-</span><span class="n">packages</span><span class="o">/</span><span class="n">keras</span><span class="o">/</span><span class="n">distribute</span><span class="o">/</span><span class="n">sidecar_evaluator</span><span class="o">.</span><span class="n">py</span><span class="p">:</span><span class="mi">17</span>
<span class="g g-Whitespace">      </span><span class="mi">1</span> <span class="c1"># Copyright 2020 The TensorFlow Authors. All Rights Reserved.</span>
<span class="g g-Whitespace">      </span><span class="mi">2</span> <span class="c1">#</span>
<span class="g g-Whitespace">      </span><span class="mi">3</span> <span class="c1"># Licensed under the Apache License, Version 2.0 (the &quot;License&quot;);</span>
   <span class="p">(</span><span class="o">...</span><span class="p">)</span>
<span class="g g-Whitespace">     </span><span class="mi">13</span> <span class="c1"># limitations under the License.</span>
<span class="g g-Whitespace">     </span><span class="mi">14</span> <span class="c1"># ==============================================================================</span>
<span class="g g-Whitespace">     </span><span class="mi">15</span> <span class="sd">&quot;&quot;&quot;Python module for evaluation loop.&quot;&quot;&quot;</span>
<span class="ne">---&gt; </span><span class="mi">17</span> <span class="kn">import</span> <span class="nn">tensorflow.compat.v2</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="g g-Whitespace">     </span><span class="mi">19</span> <span class="c1"># isort: off</span>
<span class="g g-Whitespace">     </span><span class="mi">20</span> <span class="kn">from</span> <span class="nn">tensorflow.python.platform</span> <span class="kn">import</span> <span class="n">tf_logging</span> <span class="k">as</span> <span class="n">logging</span>

<span class="ne">ModuleNotFoundError</span>: No module named &#39;tensorflow&#39;
</pre></div>
</div>
</div>
</div>
<p><strong>EXERCISE 3</strong>: Of the three implementations of neural networks explored here (from scratch, Scikit-Learn, and Keras), which implementation do you think is best?  Consider not just the accuracy of the neural network but also its runtime and ease of use.</p>
<p>Delete this text and type your response here.</p>
</section>
</section>
<section id="practice-what-you-have-learned">
<h2>Practice What You Have Learned<a class="headerlink" href="#practice-what-you-have-learned" title="Permalink to this headline">#</a></h2>
<p>Go back to Exercise 2 where you brainstormed ways to improve the performance of the neural network.  Choose three of these ideas that you think are the most promising and implement them one at a time below the cell.  Copy and paste as much code from above as needed.  Record how your changes effect the error when predicting the test data set.  Were you able to get the error significantly lower than the notebook’s result?</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./DSECOP/Solving_Differential_Equations_with_NNs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#what-is-a-neural-network">What is a neural network?</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-terminology">Neural Network Terminology</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fully-connected-feedforward-neural-network-fully-connected-ffnn">Fully Connected Feedforward Neural Network (Fully Connected FFNN)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematics-of-a-neuron">Mathematics of  a Neuron</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#activation-functions">Activation Functions</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-note-on-hyperparameters">A Note on Hyperparameters</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematics-of-neural-networks-the-first-hidden-layer">Mathematics of Neural Networks: The First Hidden Layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematics-of-neural-networks-the-second-hidden-layer">Mathematics of Neural Networks: The Second Hidden Layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematics-of-neural-networks-the-l-th-hidden-layer">Mathematics of Neural Networks: The l-th Hidden Layer</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-network-loss-function">Neural Network Loss Function</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#finding-the-optimized-weights-and-biases">Finding The Optimized Weights and Biases</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#forward-pass">Forward Pass</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#backpropagation">Backpropagation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#creating-a-neural-network-from-scratch-using-jax">Creating a Neural Network from Scratch Using JAX</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#generate-the-data-set">Generate the Data Set</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#perform-a-train-test-split">Perform a Train-Test Split</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-neural-network">Define the Neural Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#define-the-loss-function">Define the Loss Function</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#train-the-neural-network">Train the Neural Network</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analyze-the-results">Analyze the Results</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#neural-networks-with-a-popular-python-library">Neural Networks with a Popular Python Library</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#keras">Keras</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#practice-what-you-have-learned">Practice What You Have Learned</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By The Jupyter Book Community
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>